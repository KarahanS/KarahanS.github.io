<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://karahans.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://karahans.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-31T15:41:29+00:00</updated><id>https://karahans.github.io/feed.xml</id><title type="html">blank</title><subtitle>&lt;A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Independence and Uncorrelated Normal Random Variables</title><link href="https://karahans.github.io/blog/20240426" rel="alternate" type="text/html" title="Independence and Uncorrelated Normal Random Variables"/><published>2024-04-26T12:00:00+00:00</published><updated>2024-04-26T12:00:00+00:00</updated><id>https://karahans.github.io/blog/post2</id><content type="html" xml:base="https://karahans.github.io/blog/20240426"><![CDATA[<h3 id="table-of-contents">Table of Contents</h3> <ul> <li><a href="#introduction">Introduction</a></li> <li><a href="#definitions">Definitions</a></li> <li><a href="#correlation-and-independence">Correlation and Independence</a></li> <li><a href="#gaussian-distribution">Gaussian Distribution</a></li> <li><a href="#uncorrelated-normal-random-variables">Uncorrelated Normal Random Variables</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>While I was studying for my Probabilistic Machine Learning class, I came across this interesting <a href="http://probability.ca/jeff/teaching/uncornor.html">post</a> about a question asked in the Comprehensive Examinations for the PhD in Statistics at Toronto University. The question is about the two normal random variables that are uncorrelated to each other, that is, \(\mathrm{Cov}{[X, Y]} = 0\). We are simply asked whether these variables are necessarily independent or not. This question made me realize an important distinction between individually (but not jointly) normally distributed random variables and jointly normally distributed random variables. Well, before we answer this question, let’s start with the essentials.</p> <h2 id="definitions">Definitions</h2> <p>Assuming we are all familiar with the concepts listed below, let’s go through the definitions once more to have a concrete understanding of what we are dealing with.</p> <ul> <li> <p><strong>Expectation</strong>: The expectation of a random variable \(X\) is defined as \(\mathbb{E}{[X]} = \int_{-\infty}^{\infty} x f_X(x) dx\), where \(f_X(x)\) is the probability density function of \(X\). Similarly, for a discrete random variable \(X\), the expectation is defined as \(\mathbb{E}{[X]} = \sum_{x} x P(X = x)\). We will be using the definition expectation to define covariance in the next point.</p> </li> <li> <p><strong>Variance</strong>: The variance of a random variable \(X\) is defined as \(\sigma_X^2 = \mathbb{E}{[(X - \mathbb{E}{[X]})^2]}\). Variance is used to measure the spread of the data points around the mean.</p> </li> <li> <p><strong>Covariance</strong>: The covariance of two random variables \(X\) and \(Y\) is defined as \(\mathrm{Cov}{[X, Y]} = \mathbb{E}{[(X - \mathbb{E}{[X]})(Y - \mathbb{E}{[Y]})]}\). Covariance is used as a measure to estimate how change in one variable affects the change in another variable.</p> </li> <li> <p><strong>Correlation</strong>: The correlation of two random variables \(X\) and \(Y\) is defined as \(\rho_{X, Y} = \frac{\mathrm{Cov}{[X, Y]}}{\sigma_X \sigma_Y}\), where \(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\) respectively. Correlation is used to measure the strength and direction of a linear relationship between two variables. Correlation can be thought of as a normalized version of covariance. Covariance has a unit of \(X \times Y\), whereas correlation is a unitless quantity.</p> </li> </ul> <blockquote> <p>In statistics, correlation refers to a <em>linear</em> relationship between the variables.</p> </blockquote> <ul> <li><strong>Independence</strong>: Two random variables \(X\) and \(Y\) are said to be independent if \(P(X = x, Y = y) = P(X = x) P(Y = y)\) for all \(x\) and \(y\). In the continuous case, probability measure is replaced with probability density.</li> </ul> <h2 id="correlation-and-independence">Correlation and Independence</h2> <p>First of all, let’s establish the relationship between independence and correlation. If two random variables are independent, then they are uncorrelated - let’s prove it first for the continuous case (proof is exactly the same for the discrete case with some minor changes).:</p> <p>If \(X\) and \(Y\) are independent continuous random variables, then \(f(X = x, Y = y) = f(X = x) f(Y = y)\) by definition. \(\mathbb{E}{[X, Y]}\) can be simplified as follows:</p> \[\begin{aligned} \mathbb{E}{[X, Y]} &amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x y f_{X, Y}(x, y) dx dy \\ &amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x y f_X(x) f_Y(y) dx dy \\ &amp;= \int_{-\infty}^{\infty} x f_X(x) dx \int_{-\infty}^{\infty} y f_Y(y) dy \\ &amp;= \mathbb{E}{[X]} \mathbb{E}{[Y]} \end{aligned}\] <p>Recall the definition of covariance, \(\mathrm{Cov}{[X, Y]} = \mathbb{E}{[X, Y]} - \mathbb{E}{[X]} \mathbb{E}{[Y]}\). If \(X\) and \(Y\) are independent, then \(\mathrm{Cov}{[X, Y]} = 0\). Great! Now comes an important question: if \(X\) and \(Y\) are uncorrelated, are they necessarily independent? The answer is no. We can disprove it by providing a counterexample. Let’s consider the following example:</p> <p>Let’s say we have a random variable \(X\) with a symmetric probability density, that is, \(p(x) = p(-x)\). Also assume that there is another random variable \(Y = f(X)\) such that \(f\) is an even function, that is, \(f(x) = f(-x)\). Now my claim is that these guys are uncorrelated but not independent. Let’s prove it:</p> <p>Since the random variable \(X\) has a symmetric probability density, we can say that \(\mathbb{E}{[X]} = 0\) (negative and positive values will cancel out each other). Let’s calculate \(\mathbb{E}{[XY]}\):</p> \[\begin{aligned} \mathbb{E}{[XY]} &amp;= \mathbb{E}{[X f(X)]} \\ &amp;= \int_{-\infty}^{\infty} x f(x) p(x) dx \\ &amp;= \int_{-\infty}^{0} x f(x) p(x) dx + \int_{0}^{\infty} x f(x) p(x) dx \\ &amp;= \underbrace {\int_{-\infty}^{0} x f(-x) p(-x) dx}_\textrm{let $u=-x$, then $du = -dx$} + \int_{0}^{\infty} x f(x) p(x) dx \;\;\;\; \text{(using symmetry)} \\ &amp;= {\int_{\infty}^{0} -u f(u) p(u) - du} + \int_{0}^{\infty} x f(x) p(x) dx \\ &amp;= {\int_{\infty}^{0} u f(u) p(u) du} + \int_{0}^{\infty} x f(x) p(x) dx \\ &amp;= -{\int_{0}^{\infty} x f(x) p(x) dx} + \int_{0}^{\infty} x f(x) p(x) dx \\ &amp;= 0 \\ \end{aligned}\] <p>We have shown that \(\mathbb{E}{[XY]} = 0\), and we know that \(\mathbb{E}{[X]} = 0\), as a result \(\mathrm{Cov}{[X, Y]} = \mathbb{E}{[XY]} - \mathbb{E}{[X]} \mathbb{E}{[Y]} = 0\).</p> <p>However, \(X\) and \(Y\) are not <em>necessarily</em> independent. Firstly, we cannot directly conclude that \(Y\) and \(X\) are dependent because \(Y\) is a function of \(X\). We can set \(f(\cdot)\) to be a constant function - which is still an even function. Therefore it is better to construct an example to show that that \(X\) and \(Y\) do not have to be independent. Think about the case when \(X \sim U[-2, 2]\) and \(Y = X^2\). Please observe that this example complies with our construction above. Now, is \(P(Y &gt; 1 \mid -1 &lt; X &lt; 1) = P(Y &gt; 1)?\). The answer is no. \(P(Y &gt; 1 \mid -1 &lt; X &lt; 1) = 0\), but \(P(Y &gt; 1) = \frac{1}{2}\). Therefore, \(X\) and \(Y\) are not independent but can be uncorrelated.</p> <blockquote> <p>Independence requires correlation to be zero. However, (linearly) uncorrelated random variables do not have to be independent.</p> </blockquote> <h2 id="gaussian-distribution">Gaussian Distribution</h2> <p>Now, to focus on the question, let’s remind ourselves univariate and multivariate Gaussian distributions. In the one-dimensional case, a random variable \(X\) is said to have a Gaussian distribution if its probability density function is given by:</p> \[f_X(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp{\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)}\] <p>where \(\mu\) is the mean and \(\sigma\) is the standard deviation of the random variable \(X\). The Gaussian distribution is also known as the normal distribution. Multivariate Gaussian, on the other hand, is a generalization of the univariate Gaussian distribution to higher dimensions. A random vector \(\mathbf{X} = [X_1, X_2, \ldots, X_d]^T\) is said to have a multivariate Gaussian distribution if its probability density function is given by:</p> \[f_{\mathbf{X}}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp{\left(-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x} - \mathbf{\mu})\right)}\] <p>where \(\mathbf{\mu} = [\mu_1, \mu_2, \ldots, \mu_d]^T\) is the mean vector, \(\Sigma\) is the covariance matrix, and we use the determinant of it. When the different dimensions of the random vector are uncorrelated, the covariance matrix is diagonal. In this case, we can set \(\Sigma = \text{diag}(\sigma_1^2, \sigma_2^2, \ldots, \sigma_d^2) = \text{diag}(\sigma^2)\). Inverse of a diagonal matrix is again a diagonal matrix with inverse of each entry: \(\Sigma^{-1} = \text{diag}(\frac{1}{\sigma_1^2}, \frac{1}{\sigma_2^2}, \ldots, \frac{1}{\sigma_d^2}) = \text{diag}(\frac{1}{\sigma^2})\). Determinant of a diagonal matrix is the product of its diagonal entries: \(det(\Sigma) = \sigma_1^2 \sigma_2^2 \ldots \sigma_d^2\). We can plug in these values to the multivariate Gaussian distribution to simplify it when the random variables are uncorrelated:</p> \[f_{\mathbf{X}}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} (\sigma_1^2 \sigma_2^2 \ldots \sigma_d^2)^{d/2}} \exp{\left( -\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \text{diag}(\frac{1}{\sigma^2}) (\mathbf{x} - \mathbf{\mu})\right)}\] <p>Now, we can actually decompose this expression into a product of univariate Gaussian distributions. Let’s start with the left part:</p> \[\begin{aligned} \frac{1}{(2\pi)^{d/2} (\sigma_1^2 \sigma_2^2 \ldots \sigma_d^2)^{d/2}} &amp;= \frac{1}{(2\pi)^{d/2} \sigma_1 \sigma_2 \ldots \sigma_d} \\ &amp;= \frac{1}{\sqrt{2\pi} \sigma_1} \frac{1}{\sqrt{2\pi} \sigma_2} \ldots \frac{1}{\sqrt{2\pi} \sigma_d} \\ &amp;= \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi} \sigma_i} \\ \end{aligned}\] <p>Great, let’s not forget that. Now, let’s focus on the exponent part:</p> \[\begin{aligned} (\mathbf{x} - \mathbf{\mu})^T \text{diag}(\frac{1}{\sigma^2}) (\mathbf{x} - \mathbf{\mu}) &amp;= \begin{bmatrix} x_1 - \mu_1 &amp; x_2 - \mu_2 &amp; \ldots &amp; x_d - \mu_d \end{bmatrix} \begin{bmatrix} \frac{1}{\sigma_1^2} &amp; 0 &amp; \ldots &amp; 0 \\ 0 &amp; \frac{1}{\sigma_2^2} &amp; \ldots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; \frac{1}{\sigma_d^2} \end{bmatrix} \begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \\ \vdots \\ x_d - \mu_d \end{bmatrix} \\ &amp;= \begin{bmatrix} x_1 - \mu_1 &amp; x_2 - \mu_2 &amp; \ldots &amp; x_d - \mu_d \end{bmatrix} \begin{bmatrix} \frac{x_1 - \mu_1}{\sigma_1^2} \\ \frac{x_2 - \mu_2}{\sigma_2^2} \\ \vdots \\ \frac{x_d - \mu_d}{\sigma_d^2} \end{bmatrix} \\ &amp;= \frac{(x_1 - \mu_1)^2}{\sigma_1^2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2} + \ldots + \frac{(x_d - \mu_d)^2}{\sigma_d^2} \\ &amp;= \sum_{i=1}^{d} \frac{(x_i - \mu_i)^2}{\sigma_i^2} \\ \end{aligned}\] <p>Inserting our findings back to the equation yields:</p> \[f_{\mathbf{X}}(\mathbf{x}) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi} \sigma_i} \exp{\left(-\frac{1}{2}\frac{(x_i - \mu_i)^2}{\sigma_i^2}\right)}\] <p>This is the product of univariate Gaussian distributions. We have shown that when the random variables are uncorrelated, the multivariate Gaussian distribution can be decomposed into a product of univariate Gaussian distributions. But what does it mean? It means that \(f_{\mathbf{X}}(\mathbf{x}) = f_{X_1}(x_1) f_{X_2}(x_2) \ldots f_{X_d}(x_d)\), which implies that the random variables are independent (this is exactly the definition of independence).</p> <blockquote> <p>If two random variables \(X\) and \(Y\) are bivariate normal (jointly normal) and uncorrelated, then they are independent.</p> </blockquote> <h2 id="uncorrelated-normal-random-variables">Uncorrelated Normal Random Variables</h2> <p>Now comes our question: if two normal random variables are uncorrelated, are they necessarily independent? The answer is <strong>no</strong>. We have shown that if two jointly normal random variables are uncorrelated, they are independent. But when we relax the assumption of joint normality, the statement does not hold. Even if two (individually) normally distributed random variables are uncorrelated, they do not have to be independent. Let’s show this with the counterexample from the original post:</p> <p>Say, we have three random variables \(X\), \(Y\) and \(Z\). Let \(X \sim \mathcal{N}(0,\,1)\), \(Z \in \{-1, 1\}\) with equal probabilities and \(Y = Z X\). Now, is \(Y\) normally distributed? Since standard normal distribution is symmetric around zero, \(Y = X\) and \(Y = -X\) are both standard normal random variables. \(Y\) is the sum of standard normal random variables with equal probabilities, which is also a standard normal random variable. Having said that, now let’s calculate the covariance of \(X\) and \(Y\):</p> \[\begin{aligned} \mathrm{Cov}{[X, Y]} &amp;= \mathbb{E}{[XY]} - \mathbb{E}{[X]} \mathbb{E}{[Y]} \\ &amp;= \mathbb{E}{[X^2 Z]} - 0 \cdot 0 \\ &amp;= \mathbb{E}{[X^2 Z]} \\ &amp;= \mathbb{E}{[X^2]} \mathbb{E}{[Z]} \\ &amp;= 1 \cdot 0 \\ &amp;= 0 \end{aligned}\] <p>So we have two (individually) normally distributed and uncorrelated random variables. Are they independent? No! \(P(Y &gt; 0.5 \mid -0.5 &lt; X &lt; 0.5) = 0\), but \(P(Y &gt; 0.5) \neq 0\). Therefore, \(X\) and \(Y\) are not independent but uncorrelated.</p> <blockquote> <p>In summary, it’s important to note that two random variables, each following a normal distribution do not always jointly adhere to a bivariate normal distribution, nor does a covariance of zero imply independence. But when we have a bivariate normal distribution, uncorrelated random variables are independent.</p> </blockquote> <p><strong>References:</strong> Rosenthal, J. S. (2005). Uncorrelated does not imply Independence. Retrieved from <a href="http://probability.ca/jeff/teaching/uncornor.html">http://probability.ca/jeff/teaching/uncornor.html</a></p>]]></content><author><name></name></author><category term="computer-science"/><category term="machine-learning"/><summary type="html"><![CDATA[Table of Contents Introduction Definitions Correlation and Independence Gaussian Distribution Uncorrelated Normal Random Variables]]></summary></entry><entry><title type="html">Bayesian Machine Learning in Practice</title><link href="https://karahans.github.io/blog/20240303" rel="alternate" type="text/html" title="Bayesian Machine Learning in Practice"/><published>2024-03-03T11:12:00+00:00</published><updated>2024-03-03T11:12:00+00:00</updated><id>https://karahans.github.io/blog/post1</id><content type="html" xml:base="https://karahans.github.io/blog/20240303"><![CDATA[<h3 id="table-of-contents">Table of Contents</h3> <ul> <li><a href="#bayesian-approach">Bayesian Approach</a></li> <li><a href="#bayesian-ml-approach-1-ensemble">Bayesian ML Approach #1: Ensemble</a></li> <li><a href="#bayesian-ml-approach-2-training-infinite-number-of-models-bayes-by-backprop">Bayesian ML Approach #2: Training Infinite Number of Models (Bayes by Backprop)</a></li> <li><a href="#bayesian-ml-approach-3-dropout">Bayesian ML Approach #3: Dropout</a></li> <li><a href="#bayesian-ml-approach-4-training-a-curve-of-an-infinite-number-of-models">Bayesian ML Approach #4: Training a Curve of an Infinite Number of Models</a></li> <li><a href="#bayesian-ml-approach-5-stochastic-weight-averaging-swag">Bayesian ML Approach #5: Stochastic Weight Averaging (SWAG)</a></li> </ul> <h2 id="bayesian-approach">Bayesian Approach</h2> <p>What is the difference between <em>Deterministic ML</em> and <em>Bayesian ML</em>?</p> <ul> <li> <p>Deterministic ML first optimizes a single model over the training set, \(\theta^*(\mathcal{D})\) where \(\mathcal{D}\) is the training data (a collection of \((Y_i, X_i)\)). Then for a test sample, it predicts the label as follows:</p> \[P(y \mid x, \mathcal{D}) = P(y \mid x, \theta^*(\mathcal{D}))\] </li> <li>We use only this single model to produce the output for input of interest. From the Bayesian perspective, this is equivalent to having a <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac</a> posterior.</li> <li>Epistemic uncertainty arises when the model encounters with a test sample that lies in the underexplored region (region that is not covered by the training dataset). In that case, we might have multiple plausible models. Since we only consider a single one in deterministic ML, we cannot represent epistemic uncertainty using deterministic ML.</li> <li>Different set of parameters maximizes the probability, meaning that there are more than one optimum models for the task - at least based on the given training set (epistemic uncertainty).</li> </ul> <p>Bayesian ML finds a distribution of models: \(p(\theta \mid \mathcal{D})\)</p> <p>Then the prediction for a specific test sample is made through <strong>Bayesian Model Averaging</strong> (BMA)/marginalization:</p> \[P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)\] <p>The idea can be simplified as follows: Let’s say there are \(3\) different models (weights) with \(0.4, 0.4\), and \(0.2\) probabilities. Models output predicted probability as \(0.3, 0.25, 0.4\) respectively. Then the expected predicted probability becomes: \(0.4 \times 0.3 + 0.4 \times 0.25 + 0.2 \times 0.4\) = \(0.3\).</p> <hr/> <p>Recipe:</p> <ol> <li>Find a clever way to calculate (approximate) posterior distribution \(p(\theta \mid \mathcal{D})\). There are lots of ways listed below.</li> <li> <p>Using the posterior, get the prediction using <strong>Bayesian Model Averaging.</strong> Basically, we are calculating the expected value of the prediction based on the distribution \(\theta \sim p(\theta \mid \mathcal{D})\). Here we can use <strong>Monte Carlo Simulation</strong> to calculate the expected value easily.</p> \[P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)\] </li> </ol> <hr/> <p>Using Bayes’ Theorem and marginalization, we know that \(p(a)=\int p(a,b)db\). Using this equality, we can derive the following:</p> \[\begin{split} P(y | x, \mathcal{D}) &amp;= \int P(y, \theta | x, \mathcal{D}) d\theta \\ &amp;= \int P(y | \theta , x, \mathcal{D}) P(\theta | x, \mathcal{D}) d\theta \\ &amp;= \int P(y | \theta , x) P(\theta | \mathcal{D}) d\theta \\ &amp;= \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta) \end{split}\] <center> <figure> <picture> <img src="/assets/posts/post1/bayesian-3.png" class="img-fluid rounded z-depth-1" width="400" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Dependencies between test sample \(\{x, y\}\), training data \(\mathcal{D}\) and the model parameters \(\theta\) can be visualized as above.</figcaption> </figure> </center> <p>Simple observations for the network:</p> <ul> <li> <p>Test label \(y\) is independent of the training data given the parameters \(w\). That is, \(p(y, \mathcal{D} \mid w) = p(y \mid w) p(\mathcal{D} \mid w)\)</p> </li> <li> <p>\(w\) is independent of the test input \(x\). But be careful! It becomes dependent on it when the \(y\) is given. You can think of it as two independent dice throws - where \(y\) represents the event = sum of the throws is equal to 4. Without \(y\), we know that the result of the first throw is independent of the result of the second throw. But if we know that \(y\) is true or false, then the result of the first throw, then the result of the second throw becomes dependent on it.</p> </li> </ul> <h2 id="bayesian-ml-approach-1-ensemble">Bayesian ML Approach #1: Ensemble</h2> <p>So, we can utilize Bayesian ML to formulate epistemic uncertainty. Great, but how can we produce the probability distribution \(P(\theta \mid D)\)? One way of doing it is using ensemble approach - where we train a model \(M\) times with different random seeds. We can directly use the original training set or sample with replacement from it for each training. Here, we train several deterministic models. Therefore at the end, our distribution \(P(\theta \mid D)\) simply becomes the sum of Dirac measures*.</p> <p>Dirac measure, given a measurable space \((\Omega, \mathcal{B})\) and \(w \in \Omega\), is a function \(\delta_w: \mathcal{B} \rightarrow \Omega\), defined as follows:</p> \[\delta_w(A) = \begin{cases} 1, \; \text{if $\omega \in A$} \\ 0, \; \text{if $\omega \notin A$} \end{cases}\] <p>There is also the Dirac delta function (not formally a function)</p> \[\delta(x)= \begin{cases} \infty, \; \text{if $x = 0$} \\ 0, \; \text{if $x \neq 0$} \end{cases}\] <p>The function \(δ(x)\) has the value zero everywhere except at \(x = 0\), where its value is infinitely large and is such that its total integral is \(1\).</p> <p><a href="https://en.wikipedia.org/w/index.php?title=Probability_distribution#Dirac_delta_representation">Dirac function is commonly used to represent discrete probability distributions in a continuous way.</a></p> <p>As a probability distribution, integral of the Dirac function over the whole domain gets equal to \(1\).</p> \[\int_{-\infty}^\infty \delta(x) dx = 1\] <p>One final property that is extremely important is the following:</p> \[\int_{-\infty}^{\infty} f(x) \delta(x - c) \,dx = f(c)\] <p>There is a proof for this property <a href="https://math.stackexchange.com/questions/73010/proof-of-dirac-deltas-sifting-property">here</a>.</p> <p>When we train ensemble of \(M\) models, we basically get the sum of Dirac measures as our probability distribution of the random variable \(\theta\). Each Dirac measure is given as \(\delta(\theta - \theta_m)\).</p> <p>When we train ensemble of \(M\) models, we basically get the sum of Dirac measures as our probability distribution of the random variable \(\theta\). Each Dirac measure is given as \(\delta(\theta - \theta_m)\).</p> \[P(\theta|D) = \frac{1}{M} \sum_m \delta(\theta - \theta_m)\] <ul> <li> <p>“Sum of diracs” should be a probability distribution, therefore its integral over real numbers must be equal to 1.</p> \[\begin{split} &amp;\int_{-\infty}^\infty \frac{1}{M} \sum_m \delta(\theta - \theta_m)d\theta \\ &amp;= \frac{1}{M} \sum_m \int_{-\infty}^\infty \delta(\theta - \theta_m)d\theta \\&amp;= \frac{1}{M} \sum_m \int_{-\infty}^\infty \delta(y)dy \\&amp;= \frac{1}{M} \sum_m 1 \\&amp;= \frac{1}{M} M \\&amp;= 1 \end{split}\] <p>Now, we can insert our posterior to the Bayesian Model Averaging:</p> \[\begin{split} P(y \mid x, \mathcal{D}) &amp;= \int{P(y \mid x, \theta) \; P(\theta \mid \mathcal{D}) \; d\theta} \\ \;\;\; \;\;\; \;\;\; \;\;\; \;\;\;\; &amp;= \int{P(y \mid x, \theta) \; \frac{1}{M} \sum_m \delta(\theta - \theta_m) \; d\theta} \\ \;\;\; \;\;\; \;\;\; \;\;\; \;\;\;\; &amp;= \frac{1}{M} \sum_m \int{P(y \mid x, \theta) \delta(\theta - \theta_m) \; d\theta} \\ \;\;\; \;\;\; \;\;\; \;\;\; \;\;\;\; &amp;= \frac{1}{M} \sum_m P(y \mid x, \theta_m) \end{split}\] <p>So, as a result, we can get the prediction by taking the average of the resulting predictions from different models.</p> </li> <li>Pros <ul> <li>Conceptually simple - run training algorithm \(M\) times.</li> <li>Applicable to a wide range of models - from Linear Regression to LLMs</li> <li>Embarrassingly parallel - easy to parallelize.</li> </ul> </li> <li>Cons <ul> <li>Perhaps not realizing the full potential of Bayesian ML (only finite #models).</li> <li>Space &amp; time complexities scale linearly with \(M\) (#models).</li> </ul> </li> </ul> <h2 id="bayesian-ml-approach-2-training-infinite-number-of-models-bayes-by-backprop">Bayesian ML Approach #2: Training Infinite Number of Models (Bayes by Backprop)</h2> <p>Of course, it is not practically possible to train “infinite” number of models. But think about this: you can represent a probability distribution of infinite number of random variables with finite number of parameters (mean and variance). This is what we’ll try to do here. This approach called <em>Bayes by Backprop</em> is first introduced in the paper <a href="https://arxiv.org/abs/1505.05424">Weight Uncertainty in Neural Networks</a>.</p> <p>So, we can approximate \(P(\theta \mid \mathcal{D})\) simply by a Gaussian distribution:</p> <p>\(P(\theta \mid \mathcal{D}) \sim \mathcal{N}(\theta \mid \mu^*(\mathcal{D}), \Sigma^*(\mathcal{D}))\) (Here since \(D\) consists of all inputs, it is multivariate Gaussian)</p> <p>However, we don’t really know if the posterior follows a Gaussian distribution. We can try to find the closest Gaussian distribution to the actual distribution of the posterior. We would like to minimize the distance between the two distributions. Which metric quantifies how much one probability distribution differs from another probability distribution? KL-Divergence!</p> \[\text{min}_{\mu, \Sigma} d(\mathcal{N}(\theta | \mu(\mathcal{D}), \Sigma(\mathcal{D})), p(\theta | \mathcal{D}))\] <p>Now, let’s derive the training objective.</p> \[\text{KL}(\mathcal{N}(\theta \mid \mu, \Sigma) \; \Vert \; p(\theta \mid \mathcal{D})) = \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid \mu, \Sigma) \log(\frac{\mathcal{N}(\theta \mid \mu, \Sigma)}{p(\theta \mid \mathcal{D})}) d\theta\] <p>First, let’s first deal with the denominator of natural logarithm.</p> <p>\(p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}\) We can consider \(\ln(\frac{1}{p(\mathcal{D})}) = - \ln(p(\mathcal{D}))\) as a constant.</p> \[= \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \Sigma) \log(\frac{\mathcal{N}(\theta | \mu, \Sigma)}{p(\mathcal{D} | \theta) p(\theta)}) + C_1\] <p>Let’s separate the logarithm:</p> \[\begin{split} &amp;= \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid \mu, \Sigma) \log(\frac{\mathcal{N}(\theta \mid \mu, \Sigma)}{p(\theta)}) d\theta - \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid \mu, \Sigma) \log( p(\mathcal{D} \mid \theta)) d\theta + C_1 \\ &amp;= \text{KL}(\mathcal{N}(\theta \mid \mu, \Sigma) \; || \; p(\theta)) - \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid \mu, \Sigma) \log( p(\mathcal{D} \mid \theta)) d\theta + C_1\\ &amp;= \text{KL}(\mathcal{N}(\theta \mid \mu, \Sigma) \; || \; p(\theta)) - \mathbb{E}_{\theta \sim \mathcal{N}(\theta \mid \mu, \Sigma)} \log(p(\mathcal{D} \mid \theta)) + C_1 \end{split}\] <p>Ok so far so good, now assume that the input data is <em>identically independently distributed</em> (each \((Y_i, X_i)\) are independent from each other). We can represent \(p(\mathcal{D} \mid \theta)\) as follows: \(p(\mathcal{D} \mid \theta) = \prod_{i} p(Y_i, X_i \mid \theta)\) When you take the logarithm: \(\ln(p(\mathcal{D} \mid \theta)) = \sum_{i} \ln( p(Y_i, X_i \mid \theta))\)</p> <p>Therefore the expectation reduces to the following:</p> \[\mathbb{E}_{\theta \sim \mathcal{N}(\theta \mid \mu, \Sigma)} \log(p(\mathcal{D}\mid \theta)) = \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i, X_i \mid \theta))\] <p>Let’s play with the probabilities here as well:</p> \[\begin{split}p(Y_i, X_i \mid \theta) &amp;= \frac{p(Y_i, X_i, \theta)}{p(\theta)}\\ &amp;=\frac{p(Y_i \mid X_i, \theta) p(X_i, \theta)}{p(\theta)} \\ &amp;= \frac{p(Y_i \mid X_i, \theta) p(X_i \mid \theta) p(\theta)}{p(\theta)} \\ &amp;= p(Y_i \mid X_i, \theta) p(X_i \mid \theta)\\ &amp;= p(Y_i \mid X_i, \theta) p(X_i)\\ &amp;= p(Y_i \mid X_i, \theta) \; C'\end{split}\] <p>\(\theta\) and \(X_i\) are independent as long as \(Y_i\) is not provided (if not convinced, please have a look at one of our previous observations). Now, insert this result to the previous expectation:</p> \[\begin{split} \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i \mid X_i, \theta) C') &amp;= \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i \mid X_i, \theta)) + \underbrace{ \log(C') \sum_i \mathbb{E}_{\theta \sim \mathcal{N}}[1]}_\textrm{$C_2$} \\ &amp;= \sum_i \mathbb{E}_{\theta \sim \mathcal{N}}\log( p(Y_i \mid X_i, \theta)) + C_2 \end{split}\] <p>Expected value of a constant is equal to that constant.</p> <p>Ok now, there is a method called <strong>Monte Carlo Simulation.</strong> Unnecessarily fancy name, but basically it says the following: To estimate the expected value of a function \(f(\theta)\) under the distribution \(\theta \sim \mathcal{N}\), simply just run the experiment \(K\) times and take the average value of \(f(\theta)\) over runs. We can approximate the last equation as follows with MC simulation:</p> <p>\(\sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i \mid X_i, \theta)) = \sum_i \frac{1}{K} \sum_k \log(p(Y_i \mid X_i, \theta_k))\) where \(\theta_k \sim \mathcal{N}(\mu, \Sigma)\)</p> <p>So, the final objective function becomes the following:</p> \[min_{\mu, \Sigma} \; \text{KL}(\mathcal{N}(\theta \mid \mu, \Sigma) \; \Vert \; p(\theta)) - \frac{1}{K} \sum_i \sum_k \log(p(Y_i \mid X_i, \theta_k)) + C\] <p>Right part can be calculated by simply running some experiments. For the left part, what is prior \(p(\theta)\)? This represents our initial idea as to the probability distribution for model parameters. Popular choice for the posterior is just a standard normal distribution: \(p(\theta) = \mathcal{N}(\vec{0}^{\,}, {I})\) (multivariate)</p> <p>Another observation here is that allowing a full covariance matrix \(\Sigma\) can cause the computation to take extremely long (there are lots of possibilities). One trick is restricting the covariance matrix to be diagonal (independent features) \(\rightarrow \Sigma = \text{diag}(\sigma^2)\) where \(\sigma^2\) is the variance vector for each feature \((\text{Var}[\theta^i])\)*.</p> <p>* \(\theta_i\) denotes the \(i^{\text{th}}\) set of parameters sampled from the Gaussian distribution. \(\theta^i\) denotes the \(i^{\text{th}}\) dimension of the vector of parameters.</p> \[\text{KL}(\mathcal{N}(\theta \mid \mu, \text{diag}(\sigma^2)) \; \Vert \; \mathcal{N}(\vec{0}^{\,}, {I})) = \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid \mu, \text{diag}(\sigma^2))\log \left(\frac{\mathcal{N}(\theta \mid \mu, \text{diag}(\sigma^2))}{\mathcal{N}(\vec{0}^{\,}, {I})} \right) d\theta \\\] <p>So, this equation boils down to the following:</p> <p>\(= \sum_i [\frac{\mu_i^2}{\sigma_i^2} + \log\sigma_i - \frac{1}{2}]\) ⇒ this behaves like a regularizer for \(\mu\) and \(\sigma\)</p> <p>Multivariate Gaussian distribution for \(k\) features (\(k\) dimensional \(\theta\) vector):</p> \[\mathcal{N}(\mu, \Sigma) = (2\pi)^{-k/2} \text{det}(\Sigma)^{-1/2} \text{exp}(-\frac{1}{2} (x-\mu)^T \Sigma^{-1}(x - \mu))\] <p>for \(\mathcal{N}(\vec{0}^{\,}, {I}) = (2\pi)^{-k/2} \text{exp}(-\frac{1}{2}x^Tx)\)</p> <p>Then the final optimization problem becomes:</p> <p>\(min_{\mu, \Sigma} \; \sum_i [\frac{\mu_i^2}{\sigma_i^2} + \log\sigma_i - \frac{1}{2}] - \frac{1}{K} \sum_i \sum_k \log(p(Y_i \mid X_i, \theta_k)) + C\) where \(\theta_k \sim \mathcal{N}(\mu, \text{diag}(\sigma^2))\)</p> <p>Now, our equations are still not Neural Network friendly - because we have to try to minimize a function that is not directly dependent on \(\mu\) and \(\Sigma\). We have to represent \(\theta\) to include both \(\mu\) and \(\Sigma\). We can use the reparameterization here.</p> <p>If \(X \sim \mathcal{N}(\mu, \sigma^2)\), then \(Z = \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)\) Since we set \(\Sigma\) to be \(\text{diag}(\sigma^2)\), all dimensions of the parameter vector \(\theta\) are independent of each other. Individually \(\theta^i \sim \mathcal{N}(\mu_i, \sigma_i^2)\) - 1D Gaussian random variable. Therefore we can perform the given parameterization trick. By the help of reparameterization, we can separate the randomness and backpropagation - this way we avoid differentiating with respect to sampling (a similar problem occurs in Stochastic Variational Inference - which is solved via either score gradient estimators or reparameterization trick).</p> <p>\(\theta = \mu + \sigma \odot \epsilon\) where \(\odot\) is the pointwise multiplication and \(\epsilon \sim \mathcal{N}(\vec{0}^{\,}, {I})\)</p> <p>This reparameterization makes the backpropagation extremely simple:</p> \[\frac{\partial \mathcal{L}}{\partial \mu_i} = \sum_j \frac{\partial \mathcal{L}}{\partial \theta_j} \frac{\partial \theta_j}{\partial \mu_i} = \frac{\partial \mathcal{L}}{\partial \theta_i}\] \[\frac{\partial \mathcal{L}}{\partial \sigma_i} = \sum_j \frac{\partial \mathcal{L}}{\partial \theta_j} \frac{\partial \theta_j}{\partial \sigma_i} = \epsilon_i \frac{\partial \mathcal{L}}{\partial \theta_i}\] <p>Usually we set \(K\) to \(1\), that is, we only sample one \(\theta\) from the distribution <strong>in each iteration</strong>. Then the second term simply becomes the cross-entropy loss:</p> <p>\(\sum_i \log(p(Y_i \mid X_i, \theta))\) is the total loss using Cross Entropy. There is one single addition we have to do: Averaging</p> <p>\(\frac{1}{N} \sum_i \log(p(Y_i \mid X_i, \theta))\) calculates the cross entropy loss (\(P(Y_i) = 1\)) for a batch of \((Y_i, X_i)\) pairs.</p> <p>One final note: Standard deviation \(\sigma\) is the square root of variance \(\sigma^2\) and therefore it must be non-negative. However, PyTorch doesn’t know this and it might produce negative \(\sigma\) as a result of the backpropagation. We can define another function for \(\sigma\) with an input allowed to be negative:</p> <p>\(\sigma = \ln(1+e^\rho)\) (softplus - \(\sigma\) is always non-negative, let the model update the value of \(\rho\))</p> <p>PyTorch code for “Training Infinite Number of Models” (\(K\) is set to \(1\), therefore only one \(\epsilon\) is created):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BayesBackpropLinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">BayesBackpropLinear</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">).</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">).</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softplus</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">eps</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softplus</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rho</span><span class="p">)</span>
        <span class="n">kl_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_likelihood</span> <span class="o">+</span> <span class="n">kl_prior</span>
</code></pre></div></div> <p>In the forward function, we multiply \(\theta\) with \(x\). Here the idea is to mimic a shallow NN consisting of input and output layers. Result of the multiplication gives us the logits (activation must be applied to get the predicted probabilities) The main idea here is this: Rather than training one parameter set \(\theta\) over each iteration, we train parameters \(\mu\) and \(\sigma\) that will generate infinitely many \(\theta\) → Bayes ML In the forward function, we multiply \(\theta\) with \(x\). Here the idea is to mimic a shallow NN consisting of input and output layers. Result of the multiplication gives us the logits (activation must be applied to get the predicted probabilities) The main idea in this code is this: Rather than training one parameter set \(\theta\) over each iteration, we train parameters \(\mu\) and \(\sigma\) that will generate infinitely many \(\theta\) → Bayes ML.</p> <p>As a result of this model, we will get \(\mu^*\) and \(\Sigma^*\) that minimizes the \(\text{KL}(\mathcal{N}(\theta \mid \mu(\mathcal{D}), \Sigma(\mathcal{D})), p(\theta \mid \mathcal{D}))\) where \(P(\theta \mid \mathcal{D}) \sim \mathcal{N}(\theta \mid \mu^*(\mathcal{D}), \Sigma^*(\mathcal{D}))\)</p> <p>What remains is the application of BMA. Calculate the expected value of \(P(y \mid x, \theta)\) using the \(\theta\) sampled from \(\mathcal{N}(\theta \mid \mu^*(\mathcal{D}), \Sigma^*(\mathcal{D}))\) using the learned mean and variance.</p> \[P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{P(\theta|D)} P(y |x, \theta)\] <h2 id="bayesian-ml-approach-3-dropout">Bayesian ML Approach #3: Dropout</h2> <p>On the spectrum of Bayesian methods, dropout is between the sum of Diracs (training a few deterministic ML models) and the variational approach (that trains an infinite number of models).</p> <p>Dropout is randomly applied to the model <strong>for each iteration.</strong> Considering the Cross Entropy Loss, objective function of the model can be formulated as follows:</p> <p>\(-\frac{1}{N}\sum_{n=1}^{N}\log P(y_n \mid x_n, s \odot \theta)\) where we turn on/off each weight dimension randomly in each iteration: \(s_i \sim \text{Bernouilli}( p)\) where \(p\) is the dropout rate. Dropout is something that people used to use a lot → Bayesian NN for free.</p> <p>Now, here is the idea: Normally, we train models with dropouts and then close the dropout at inference time (test time). Here, we can still apply dropout to have different models make predictions during test time. Again, we apply Bayesian Model Averaging with Monte Carlo estimation for the integral.</p> \[\begin{split}P(y \mid x, \mathcal{D}) &amp;= \int{P(y \mid x, \theta) \; P(\theta \mid \mathcal{D}) \; d\theta} \\ &amp;= \mathbb{E}_{\theta \sim P(\theta \mid D)} P(y \mid x, \theta) \\ &amp;\approx \frac{1}{K} \sum_{k=1}^K P(y \mid x, \theta_{k}) \;\; \text{where} \;\; \theta_{k} = s_k \odot \theta \end{split}\] <p>There are \(K\) models to compare with each other. We expect to have disagreements between the models for OOD samples, and therefore measure the epistemic uncertainty.</p> <p>Problem: Unfortunately, this method doesn’t work that well in practice as shown in the paper <a href="https://arxiv.org/abs/1612.01474">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>.</p> <center> <figure> <picture> <img src="/assets/posts/post1/bayesian-6.png" class="img-fluid rounded z-depth-1" width="400" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">R: Random signed vector added - AT: Adversarial training added</figcaption> </figure> </center> <h2 id="bayesian-ml-approach-4-training-a-curve-of-an-infinite-number-of-models">Bayesian ML Approach #4: Training a Curve of an Infinite Number of Models</h2> <p>This idea makes the following assumption: If you train two models independently, you get two different \(\theta_i\) weights that are far away from each other in the space. But from one to another, there is a path where the loss value is always low. So we can find the weights in between to have infinite number of models.</p> <p>This is also Bayesian, as we are training an infinite number of models according to a learned parametric approximate posterior.</p> <p>Here is the algorithm to train a curve of infinite number of models:</p> <ol> <li>Train two independent models: \(\theta_1\) and \(\theta_2\)</li> <li>Parameterize the curve via a third model \(\phi\). There are two line segments between the models.</li> </ol> \[\theta_{\phi}(t) = \begin{cases} 2(t\phi + (0.5-t)\theta_1)\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if}\; t \in [0, 0.5) \\ 2((t - 0.5)\theta_2 + (1-t)\phi) \;\;\;\; \text{if} \; t \in [0.5, 1] \end{cases}\] <p>\(\theta_1\) and \(\theta_2\) are already known, we just have to find \(\phi\) so that the any model on the curve has low training loss. So, we actually have to minimize the expected value of the cross entropy loss:</p> <p>\(\text{min}_\phi \;\mathbb{E}_{\theta \sim q_{\phi}(\theta)} \left[ -\frac{1}{N} \sum_n \log P(y_n \mid x_n, \theta) \right]\) → find a suitable \(\phi\) under the expectation of both \(\theta\) and \(X, Y\)</p> <p>Using the reparameterization trick:</p> \[\text{min}_\phi \;\mathbb{E}_{t \sim \text{Unif}[0, 1]}\left[ -\frac{1}{N} \sum_n \log P(y_n | x_n, \theta_{\phi}(t)) \right]\] <p>This minimization aims to minimize the cross entropy loss by finding a suitable \(\phi\).</p> <blockquote> <p>A general observation (<a href="https://arxiv.org/pdf/1802.10026.pdf">Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</a>) is that almost all pairs of independently trained models (\(\theta_1, \theta_2\)) for DNNs are connected through a third point \(\phi\) in a low-loss “highway” that we can easily find. This gives an interesting intuition for the loss landscape: Most solutions in the DL landscape are connected by some piecewise linear curve. This is not so surprising: We have millions/billions of dimensions to choose from. We can likely find a 2D cut of the loss in which there exists a parametric curve parameterized by \(\phi\) that connects the two endpoints with a low training loss.</p> </blockquote> <p>After training the model and finding a suitable \(\phi\), the rest is using the <strong>Bayesian Model Averaging</strong> by sampling models (weights) from the curve between \(\theta_1\) and \(\theta_2\):</p> \[\begin{split} P(y \mid x, \mathcal{D}) &amp;= \int{P(y \mid x, \theta) \; P(\theta \mid \mathcal{D}) \; d\theta} \\ &amp;= \mathbb{E}_{\theta \sim P(\theta|D)} P(y \mid x, \theta) \\ &amp;\approx \frac{1}{K} \sum_{k=1}^K P(y | x, \theta_\phi{(t^{(k)})}) \;\; \text{where} \;\; t^{(k)} \sim \text{Unif}[0, 1] \end{split}\] <h2 id="bayesian-ml-approach-5-stochastic-weight-averaging-swag">Bayesian ML Approach #5: Stochastic Weight Averaging (SWAG)</h2> <p>Lastly, we can exploit the randomness in SGD as a cheap source for Bayesian ML. Because of small batches (usually batch size = 1), SGD trajectory is very noisy. The training’s final few iterations (epochs) can be treated as samples from the approximate posterior distribution \(p(\theta \mid \mathcal{D})\).</p> <p>We would like to approximate the last iterations with Gaussian:</p> \[p(\theta | \mathcal{D}) \sim \mathcal{N}(\theta | \mu(\mathcal{D}), \Sigma(\mathcal{D}))\] <p>We assume that last \(\theta_i\) values are coming from a Gaussian distribution. We want to find the mean and the covariance matrix for it. We can use the sample estimates basically:</p> \[\mu = \frac{1}{L} \sum_l \theta_l\] <p>\(\Sigma = \text{diag} \left( \frac{1}{L} \sum_l \theta_l^2 - (\frac{1}{L} \sum_l \theta_l)^2 \right)\) assuming that the different features \(\theta_i\) and \(\theta_j \; (i \neq j)\) are not correlated with each other.</p> <p>SWAG is not so scalable because it requires the full empirical covariance matrix. On the other hand SWAG-Diag can be used to reduce the time complexity. However, according to the benchmark results, it seems that SWAG-Diag only scales better computationally, but the results do not follow.</p> <p>After getting the \(p(\theta \mid \mathcal{D})\), rest is similar to what we have done so far:</p> <ol> <li> <p>Using the posterior, get the prediction using <strong>Bayesian Model Averaging</strong>. Basically, we are calculating the expected value of the prediction based on the distribution \(\theta \sim p(\theta \mid \mathcal{D})\).</p> \[P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)\] </li> <li> <p>To approximate the epistemic uncertainty, either calculate the entropy \(\mathbb{H}(P(y \mid x, \mathcal{D}))\) or the max-prob for classification \(\text{max}_k P(Y = k \mid x, \mathcal{D})\).</p> </li> </ol> <hr/> <p><strong>Conclusion</strong>: There are strong assumptions for these Bayesian approaches to work:</p> <ul> <li>We should have a reasonable prior for the parameters \(\theta\).</li> <li>The posterior follows the assumed distribution (e.g., a Gaussian). Of course, the posterior will seldom be truly Gaussian. This is a huge assumption.</li> </ul> <hr/> <p><strong>References:</strong> Bálint Mucsányi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): “Proper/Strictly Proper Scoring Rule”; in: Trustworthy Machine Learning; URL: https://trustworthyml.io/; DOI: 10.48550/arXiv.2310.08215.</p>]]></content><author><name></name></author><category term="computer-science"/><category term="machine-learning"/><summary type="html"><![CDATA[Table of Contents Bayesian Approach Bayesian ML Approach #1: Ensemble Bayesian ML Approach #2: Training Infinite Number of Models (Bayes by Backprop) Bayesian ML Approach #3: Dropout Bayesian ML Approach #4: Training a Curve of an Infinite Number of Models Bayesian ML Approach #5: Stochastic Weight Averaging (SWAG)]]></summary></entry><entry><title type="html">Scoring Rules</title><link href="https://karahans.github.io/blog/20240201" rel="alternate" type="text/html" title="Scoring Rules"/><published>2024-02-01T11:12:00+00:00</published><updated>2024-02-01T11:12:00+00:00</updated><id>https://karahans.github.io/blog/post3</id><content type="html" xml:base="https://karahans.github.io/blog/20240201"><![CDATA[<h3 id="table-of-contents">Table of Contents</h3> <ul> <li><a href="#1-introduction">Introduction</a></li> <li><a href="#2-definitions">Definitions</a> <ul> <li><a href="#21-scoring-rule">Scoring Rule</a></li> <li><a href="#22-proper-scoring-rule">Proper Scoring Rule</a></li> <li><a href="#23-strictly-proper-scoring-rule">Strictly Proper Scoring Rule</a></li> </ul> </li> <li><a href="#3-scoring-rules">Scoring Rules</a> <ul> <li><a href="#31-log-probability">Log Probability</a></li> <li><a href="#32-brier-score">Brier Score</a></li> </ul> </li> <li><a href="#4-proofs">Proofs</a> <ul> <li><a href="#41-theorem-log-probability-scoring-rule">Theorem (Log Probability Scoring Rule)</a></li> <li><a href="#42-theorem-brier-scoring-rule">Theorem (Brier Scoring Rule)</a></li> </ul> </li> <li><a href="#5-application-predictive-certainty">Application: Predictive Certainty</a> <ul> <li><a href="#51-max-prob-confidence">Max-Prob Confidence</a></li> <li><a href="#52-bce-loss">BCE Loss</a></li> </ul> </li> <li><a href="#conclusion">Conclusion</a></li> </ul> <h2 id="1-introduction">1) Introduction</h2> <p>Scoring rules are used to evaluate the performance of probabilistic forecasts. Roughly, a scoring rule is a function that maps a forecast and an observation to a real number, which is interpreted as a measure of the quality of the forecast. For a rational agent who wants to maximize the expected score, the best forecast is the one that is most likely to be the true observation. In this post, we will discuss a couple of definitions related to scoring rules . Then we will examine some of the commonly used proper scoring rules in the literature. At the end we’ll briefly discuss the relationship between scoring rules and predictive certainty.</p> <h2 id="2-definitions">2) Definitions</h2> <h5 id="21-scoring-rule">2.1) Scoring Rule</h5> <p>A scoring rule is any extended real-valued function \(\mathbf{S}: \mathcal{Q} \times \Omega \rightarrow \mathbb {R}\) where \(\mathcal{Q}\) is a family of probability distributions over the space \(\Omega\), such that \(\mathbf{S}(Q, \cdot)\) is \(\mathcal{Q}\)-quasi-integrable* for all \(Q \in \mathcal{Q}\). Output of the function \(\mathbf{S}(Q, y)\) represents the loss or penalty when the forecast \(Q \in \mathcal{Q}\) is issued and the observation \(y \in \Omega\) is realized.</p> <p>*In our definition, we use the term “quasi-integrable” to mean that the function is integrable with respect to the measure induced by the probability distribution. This is just a technical condition that ensures the existence of the expected score. So, nothing to worry about here.</p> <h5 id="22-proper-scoring-rule">2.2) Proper Scoring Rule</h5> <p>A scoring rule \(\mathbf{S}\) is called a <strong>proper scoring rule</strong>, if and only if</p> \[\label{eq:psr} \max_{Q \in \mathcal{Q}} \mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = \mathbb{E}_{Y \sim P}[\mathbf{S}(P, Y)] \; .\] <p>In other words, score function \(\mathbf{S}\) is a proper scoring rule, if it is maximized when the forecaster gives exactly the ground truth distribution \(P(Y)\) as its probabilistic forecast \(Q \in \mathcal{Q}\).</p> <h5 id="23-strictly-proper-scoring-rule">2.3) Strictly Proper Scoring Rule</h5> <p>A scoring rule \(\mathbf{S}\) is called a <strong>strictly proper scoring rule</strong>, if and only if</p> <ul> <li> <p>\(\mathbf{S}\) is a proper scoring rule, and</p> </li> <li> <p>\(\operatorname*{arg\,max}_{Q \in \mathcal{Q}} \; \mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = P\) is the unique maximizer of \(\mathbf{S}\) in \(Q\).</p> </li> </ul> <p>In other words, a strictly proper scoring rule is maximized only when the the forecaster gives exactly the ground truth distribution \(P(Y)\) as its probabilistic forecast \(Q \in \mathcal{Q}\).</p> <p>So, there is a slight difference between <em>proper scoring rules</em> and <em>strictly proper scoring rules</em>. In an ideal scenario, we want our scoring rule to be strictly proper so that it awards the maximum reward only to the forecaster who gives the true distribution as its forecast.</p> <h2 id="3-scoring-rules">3) Scoring Rules</h2> <h5 id="31-log-probability">3.1) Log Probability</h5> <p>A log (probability) scoring rule \(S(q, y)\) is as a scoring rule that measures the quality of a probabilistic forecast in decision theory. Formally, it can be defined in discrete or continuous form as follows:</p> <p>1) Log scoring rule for binary classification:</p> \[S(q, y) = \left\{ \begin{array}{rl} \log q \; , &amp; \text{if} \; y = 1 \\ \log(1-q) \; , &amp; \text{if} \; y = 0 \end{array} \right.\] <p>which can be expressed as</p> \[\label{eq:binary-lpsr} S(q, y) = y \log q + (1-y) \log (1-q)\] <p>Note that the expressions given above have slightly different domains. For the first equation, the domain is \(D_1 = ([0,1) \times \left\lbrace 0 \right\rbrace) \cup ((0, 1] \times \left\lbrace 1 \right\rbrace)\), while for the second equation, the domain is \(D_2 = (0,1) \times \left\lbrace 0,1 \right\rbrace\).</p> <p>2) Log scoring rule for multiclass classification:</p> \[\label{eq:multiclass-lpsr} S(q, y) = \sum_k y_k \log q_k(x) = \log q_{y^*}(x)\] <p>where \(y^*\) is the true class and \(q\) is the predicted probability distribution over the classes. We have \(y_k = 1\), if the true class is \(k\) and \(y_k = 0\) otherwise.</p> <p>3) Log scoring rule for regression (continuous case):</p> \[\label{eq:regression-lpsr} S(q, y) = \log q(y)\] <p>where \(q\) is the predicted probability distribution over the continuous space and \(y\) is the true value.</p> <h5 id="32-brier-score">3.2) Brier Score</h5> <p>1) Brier score for binary classification:</p> \[S(q, y) = -(q - y)^2\] <p>\(q\) represents the predicted probability of the positive class (\(Y = 1\)) and \(y\) is the true class label. Since we want the output of the scoring rule to be maximized when the predicted probability is close to the true class label, we use the negative of the squared difference between the predicted probability and the true class label.</p> <p>2) Brier score for multiclass classification:</p> \[S(q, y) = -\sum_k (q_k - y_k)^2 = -(q_{y^*} - 1)^2 -\sum_{k \neq y^*} q_k^2\] <p>where \(q_k\) is the predicted probability of the class \(y^*\) is the true class label. Similar to the log probability score, we have \(y_k = 1\), if the true class is \(k\) and \(y_k = 0\) otherwise.</p> <p>Although there is no direct version of Brier score for regression, we can use the squared error loss as a scoring rule for regression problems.</p> <h2 id="4-proofs">4) Proofs</h2> <p>Having defined the scoring rules and given examples for them, we can now prove that those scoring rules are strictly proper. We will start with the log probability scoring rule.</p> <h5 id="41-theorem-log-probability-scoring-rule">4.1) Theorem (Log Probability Scoring Rule)</h5> <p><strong>Theorem:</strong> The log (probability) scoring rule is a strictly proper scoring rule.</p> <p><strong>Proof:</strong> We will show that all versions of the log probability scoring rule (binary/multiclass/regression) are strictly proper scoring rules.</p> <p>1) Binary log probability scoring rule:</p> \[\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = P(Y = 1) \log q + P(Y = 0) \log (1 - q)\] <p>Let \(p\) be the true probability of the event \(Y = 1\). Then, the expected score is:</p> \[\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = p \log q + (1 - p) \log (1 - q)\] <p>To find the maxima, take the derivative with respect to \(q\) and set it to zero:</p> \[\begin{split} \frac{\partial}{\partial q}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= \frac{p}{q} - \frac{1-p}{1 - q} \\ 0 &amp;= \frac{p - pq - q + pq}{q (1-q)} \\ 0 &amp;= \frac{p - q}{q (1-q)} \\ \Rightarrow p - q &amp;= 0 \\ \Rightarrow p &amp;= q \end{split}\] <p>Now, we need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:</p> \[\begin{split} \frac{\partial^2}{\partial q^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= -\frac{p}{q^2} - \frac{1-p}{(1 - q)^2} \\ &amp;= - \left( \underbrace{\frac{p}{q^2}}_\textrm{&gt; 0} + \underbrace{\frac{1-p}{(1 - q)^2}}_\textrm{&gt; 0} \right) &lt; 0 \end{split}\] <p>Except for the cases \(q=0\) and \(q=1\), the second derivative is always negative, which means that the function is concave and the maximum is unique. For \(q = 1\), maximum is achieved only if \(p = 1\), and similarly for \(q = 0\), maximum is achieved only if \(p = 0\). Therefore, \(p = q\) is the only maximizer and the log probability scoring rule for binary classification is strictly proper.</p> <p>2a) Multiclass log probability scoring rule (Proof #1):</p> \[S(q, y) = \sum_k^K y_k \log q_k(x)\] <p>Let \(p_k\) be the true probability of the event \(Y = k\). Since \(q_k\) is the predicted probability for class \(k\), we know that \(\sum_i q_i = 1\). Then, the expected score is:</p> \[\begin{split} \mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= \sum_k P(Y = k|x) \log(q_k(x)) \\ &amp;= p_1 \log(q_1(x)) + p_2\log(q_2(x)) + ... + p_K \log(q_K(x)) \\ &amp;= p_1 \log(q_1(x)) + p_2\log(q_2(x)) + ... + p_K \log(1 - \sum_{i \neq K} q_i(x)) \end{split}\] <p>Taking the derivative with respect to \(q_j\) and setting it to zero:</p> \[\begin{split} \frac{\partial}{\partial q_j}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= \frac{p_j}{q_j} -\frac{p_K}{1- \sum_{i \neq K} q_i(x)} \\ 0 &amp;= \frac{p_j}{q_j} - \frac{p_K}{q_K} \\ \Rightarrow \frac{p_j}{q_j} &amp;= \frac{p_K}{q_K} \end{split}\] <p>This equality holds for any \(j\):</p> \[\frac{p_1}{q_1} = \frac{p_2}{q_2} = ... = \frac{p_K}{q_K} = \lambda\] <p>Each \(q_i\) can be represented as a constant multiple of \(p_i\) as follows: \(q_i = \lambda \ p_i\)</p> \[\begin{split} \sum_i q_i &amp;= 1 \\ \sum_i \lambda \ p_i &amp;= 1 \\ \lambda \sum_i p_i &amp;= 1 \\ \lambda = 1 \end{split}\] <p>Since \(\lambda = 1\), we have \(p_i = q_i\) for all \(i\). Now, we need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:</p> \[\begin{split} \frac{\partial^2}{\partial q_j^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= -\frac{p_j}{q_j^2} - \frac{p_K}{(1- \sum_{i \neq K} q_i(x))^2} \\ &amp;= - \left( \underbrace{\frac{p_j}{q_j^2}}_\textrm{&gt; 0} + \underbrace{\frac{p_K}{(1- \sum_{i \neq K} q_i(x))^2}}_\textrm{&gt; 0} \right) &lt; 0 \end{split}\] <p>Except for the cases \(q_j=0\) and \(q_j=1\), the second derivative is always negative, which means that the function is concave and the maximum is unique. For \(q_j = 1\), maximum is achieved only if \(p_j = 1\), and similarly for \(q_j = 0\) maximum is achieved only if \(p_j = 0\). Therefore, \(p_j = q_j\) is the only maximizer and the log probability scoring rule for multiclass classification is strictly proper.</p> <p>2b) Multiclass log probability scoring rule (Proof #2):</p> <p>Alternatively, we can solve the optimization problem with Lagrange multipliers. The Lagrangian is:</p> \[\mathcal{L}(q, \lambda) = \sum_k P(Y = k|x) \log(q_k(x)) + \lambda \left(1 - \sum_k q_k(x)\right)\] <p>Taking the derivative with respect to \(q_j\) and setting it to zero:</p> \[\begin{split} \frac{\partial}{\partial q_j}\mathcal{L}(q, \lambda) &amp;= \frac{p_j}{q_j} - \lambda \\ 0 &amp;= \frac{p_j}{q_j} - \lambda \\ \Rightarrow \frac{p_j}{q_j} &amp;= \lambda \end{split}\] <p>The rest of the proof follows as in the first proof.</p> <p>3) Continuous log probability scoring rule:</p> \[S(q, y) = \log q(y)\] <p>Let \(p(y)\) be the true probability density function of the event \(Y = y\). Then, the expected score is:</p> \[\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = \int p(y) \log q(y) dy\] <p>Let \(X = \frac{q(y)}{p(y)}\) and \(\phi = \log(\cdot)\) (a concave function). By Jensen’s inequality, we know that \(f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]\), if \(f\) is concave. Therefore, we have:</p> \[\begin{split} \int p(y) \log \frac{q(y)}{p(y)} dy &amp;\leq \log \int p(y)\frac{q(y)}{p(y)} dy \\ \int p(y) \log \frac{q(y)}{p(y)} dy &amp;\leq \log \int q(y) dy \\ \int p(y) \log \frac{q(y)}{p(y)} dy &amp;\leq \log(1) \\ \int p(y) \log \frac{q(y)}{p(y)} dy &amp;\leq 0 \end{split}\] <p>The same result can be obtained by using the Kullback-Leibler divergence. The Kullback-Leibler divergence is always non-negative, therefore \(\text{E} - \text{CE} = \text{KL} \geq 0\). The resulting expression is \(-\text{KL}\), which is always non-positive. It is maximized only when \(q(y) = p(y)\) which means that the log probability scoring rule for continuous classification is strictly proper.</p> <p>An alternative argument for uniqueness of the maximum point can be proposed as follows: \(\int p(y) \log \frac{q(y)}{p(y)} dy\) can be equal to \(0\) in two cases: Either \(\frac{q(y)}{p(y)}\) is equal to \(1\) for each value or the expression \(\log ( \frac{q(y)}{p(y)})\) takes positive and negative values summing up to \(0\) at the end. The second case cannot occur, because it means that there exists a \(y_0\) such that \(q(y_0) &gt; p(y_0)\), implying that Jensen’s inequality is violated. Therefore, the maximum is achieved, if and only if \(q = p\).</p> <h5 id="42-theorem-brier-scoring-rule">4.2) Theorem (Brier Scoring Rule)</h5> <p><strong>Theorem:</strong> The Brier scoring rule is a strictly proper scoring rule.</p> <p><strong>Proof:</strong> We will show that the Brier scoring rule is strictly proper for both binary and multiclass classification.</p> <p>1) Binary Brier Scoring rule:</p> \[\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = - P(Y = 1) (q - 1)^2 + P(Y = 0) -q^2\] <p>Let \(p\) be the true probability of the event \(Y = 1\). Then, the expected score is:</p> \[\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = - p (q - 1)^2 - (1 - p) q^2\] <p>To find the maxima, take the derivative with respect to \(q\) and set it to zero:</p> \[\begin{split} \frac{\partial}{\partial q}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= -2p(q - 1) - 2(1 - p)q \\ &amp;= -2pq + 2p - 2q + 2pq \\ &amp;= 2p - 2q \\ 0 &amp;= 2p - 2q \\ \Rightarrow p &amp;= q \end{split}\] <p>We need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:</p> \[\begin{split} \frac{\partial^2}{\partial q^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= -2 &lt; 0 \\ \end{split}\] <p>Second derivative is always negative, which means that the function is concave and the maximum is unique. Therefore, \(p = q\) is the only maximizer and the Brier scoring rule for binary classification is strictly proper.</p> <p>2) Multiclass Brier Scoring rule:</p> \[\begin{split} \mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= \sum_k P(Y = k) \bigg[ -\sum_i (q_i - y_i)^2 \bigg]\\ &amp;= \sum_k P(Y = k) \bigg[ -(q_{k} - 1)^2 -\sum_{i \neq k} q_i^2 \bigg] \\ &amp;= \sum_k P(Y = k) \bigg[ -(q_{k} - 1)^2 + q_k^2 -\sum_{i} q_i^2 \bigg] \\ &amp;= \sum_k P(Y = k) \bigg[ -q_{k}^2 - 1 + 2q_k + q_k^2 -\sum_{i} q_i^2 \bigg] \\ &amp;= \sum_k P(Y = k) \bigg[ 2q_k - 1 -\sum_{i} q_i^2 \bigg] \\ &amp;= \sum_k P(Y = k)(2q_k - 1) - \sum_k P(Y = k) \bigg(\sum_i q_i^2\bigg) \\ &amp;= \sum_k P(Y = k)(2q_k - 1) - \sum_i q_i^2 \bigg(\underbrace{\sum_k P(Y = k)}_\textrm{1}\bigg) \\ &amp;= \sum_k P(Y = k)(2q_k - 1) - \sum_i q_i^2 \\ &amp;= \sum_k P(Y = k)(2q_k - 1) - q_k^2 \end{split}\] <p>Similar to what we did for log probability, this expression can be expressed as follows (replacing \(q_K\) with \(1 - \sum_{i \neq K} q_i\)):</p> \[\begin{split} \mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= p_1(2q_1 - 1) - q_1^2 + p_2(2q_2 - 1) - q_2^2 + ... + p_K(2q_K - 1) - q_K^2 \\ &amp;= p_1(2q_1 - 1) -q_1^2 + p_2(2q_2 - 1) -q_2^2 + ... + p_K(1 - 2\sum_{i \neq K} q_i) - (1 - \sum_{i \neq K} q_i)^2\\ \end{split}\] <p>Taking the derivative with respect to \(q_j\) and setting it to zero:</p> \[\begin{split} \frac{\partial}{\partial q_j}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= 2p_j - 2q_j - 2p_K + 2 (1 - \sum_{i \neq K} q_i) \\ &amp;= 2p_j - 2q_j - 2p_K + 2 q_K \\ &amp;= (p_j - q_j) + (q_K - p_K) \\ (p_j - q_j) &amp;= (p_K - q_K) \\ \end{split}\] <p>For all derivaties, the following equality holds:</p> \[p_1 - q_1 = p_2 - q_2 = ... = p_K - q_K = \lambda\] <p>We know that \(\sum_i q_i = 1\) and \(\sum_i p_i = 1\), therefore:</p> \[\begin{split} \sum_i p_i - q_i &amp;= K \cdot \lambda = 0 \\ \Rightarrow \lambda &amp;= 0\\ \Rightarrow p_i &amp;= q_i \end{split}\] <p>Now, we need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:</p> \[\begin{split} \frac{\partial^2}{\partial q_j^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &amp;= - 2 - 2 &lt; 0 \\ \end{split}\] <p>The second derivative is always negative, which means that the function is concave and the maximum is unique. Therefore, \(p = q\) is the only maximizer and the Brier scoring rule for multiclass classification is strictly proper.</p> <h2 id="5-application-predictive-certainty">5) Application: Predictive Certainty</h2> <p>Ok, so far so good. We have seen that there is a concept called scoring rule, and we have seen that log probability and Brier scoring rules are strictly proper. But, why do we care about scoring rules? There are lots of different application areas for scoring rules, but among them, one of the most important ones is predictive certainty. Predictive certainty is the measure of how certain the model is about its predictions. In other words, it is the measure of how much the model trusts its predictions. Scoring rules can be used to measure the predictive certainty of a model.</p> <h5 id="51-max-prob-confidence">5.1) Max-Prob Confidence</h5> <p>How can one measure the predictive certainty? Well, let’s say we are dealing with a binary classification problem. Our predicted output can take values between 0 and 1. Here, intuitively we say that our model is certain about its prediction (as positive) when it is very close to 1, and again it is certain about its prediction (as negative) when it is very close to 0. Therefore, we can use the predicted probability as a measure of the predictive certainty. In other words, we can use the predicted probability as a measure of how much the model trusts its predictions. This is called <strong>max-prob confidence</strong>.</p> <p>Let’s define a random variable \(L\) which represents the correctness of our prediction.</p> \[\begin{equation*} L = \begin{cases} 1 &amp;\text{if prediction is correct}\\ 0 &amp;\text{otherwise} \end{cases} \end{equation*}\] <p>Then \(P(L = 1)\) corresponds to the probability that our prediction is correct. In the deployment environment, we don’t really know if our prediction is wrong or not (otherwise we wouldn’t need to make a prediction at all). Our aim is to get the most reliable “estimate” for \(P(L=1)\) so that we really know how confident our model is about its predictions. For a real life scenario, think about a doctor who is trying to diagnose a patient. Diagnosing a patient who is not really sick (false positive), may not be a huge problem (except the stress and the cost). But, <strong>not</strong> diagnosing a patient who is really sick (false negative), is a serious problem. Therefore, the doctor should be very confident about his/her diagnosis. An ML algorithm tailored for analyzing medical documents might conclude that the patient is in good health. If the doctor is provided with the confidence level of the algorithm, he/she can decide whether to trust the algorithm or not. If the confidence level is high, the doctor may trust the algorithm. If the confidence level is low, the doctor may attempt to run some additional tests to make sure that the patient is not sick.</p> <p>We want to reward the “reporter” of the confidence level if the reported confidence is close to the real confidence \(P(L=1)\). Does it sound familiar? So, here is the part where scoring rules come into play. We can use a proper scoring rule to <em>encourate</em> the reporter to provide confidence levels as close as possible to the real (ground-truth) confidence level \(P(L=1)\).</p> <h5 id="52-bce-loss">5.2) BCE Loss</h5> <p>Here’s the good news! Majority of the traditional loss functions we employ during the training of our models are already proper scoring rules for predicting the confidence level using max-prob confidence. In a sense, they do not only aim to achieve highest accuracy, but also the truthfulness of the max-prob confidence. So, if an algorithm predicts 0.9 for its prediction, we can be sure that this prediction is more likely to be true. Here, it’s important not to conflate the concepts of “accuracy” and “confidence.” Our algorithm might demonstrate high accuracy but lack confidence in its predictions, perhaps consistently outputting probabilities around 0.55 for positive cases and 0.45 for negative cases. Conversely, it could exhibit high confidence despite lower accuracy; for instance, consistently outputting around 0.9 for negative samples, despite this leading to incorrect predictions. We do not want our model to be underconfident or overconfident.</p> <p>Now, let’s show that one of the most commonly used loss functions, the negative of Binary Cross-Entropy (BCE) loss, is also a proper scoring rule for max-prob confidence. Since loss is usually minimized and score is maximized, we use the negative of BCE loss as a scoring rule.</p> <blockquote> <p>Careful! It is not that using BCE loss aims to maximize the reported confidence level. Both overconfidence and underconfidence are undesired calibration errors. The aim is to <strong>minimize the loss</strong> and <strong>maximize the score</strong> (latter means encouraging the reporter to provide confidence levels as close as possible to the real confidence level \(P(L=1)\)).</p> </blockquote> <p>Let’s start from the beginning. Now, we want to create a game where there is a reporter of the confidence and the ground truth confidence level \(P(L=1)\). We would like to make sure that the reported confidence level is as close as possible to the real confidence level. We have to design a scoring rule to reward the reporter. We know that log probability score is a strictly proper scoring rule, therefore it is only maximized when the reporter gives the true distribution as its probabilistic forecast. Let \(c(x) = \max(f(x), 1 - f(x))\) and remember the binary log probability scoring rule:</p> \[\begin{equation*} S(c, L) = \begin{cases} \log(c(x)) &amp;\text{if $L$ = 1}\\ \log(1 - c(x)) &amp;\text{if $L$ = 0} \end{cases} \end{equation*}\] <p>We know that \(c = p(L)\) is the unique maximizer of this scoring function. Let’s evaluate the cases:</p> <ol> <li>\(Y = 0\) and \(\hat{Y} = 0\): Our prediction is correct, scoring function outputs \(\log(1-f(x))\)</li> <li>\(Y = 0\) and \(\hat{Y} = 1\): Our prediction is wrong, scoring function outputs \(\log(1 - f(x))\)</li> <li>\(Y = 1\) and \(\hat{Y} = 0\): Our prediction is wrong, scoring function outputs \(\log(f(x))\)</li> <li>\(Y = 1\) and \(\hat{Y} = 1\): Our prediction is correct, scoring function outputs \(\log(f(x))\)</li> </ol> <p>Let’s re-formulate the scoring function:</p> \[S(f, Y) = \begin{cases} \log (f(x)) \;\;\;\;\;\;\;\;\;\;\;\;\text{if}\; Y = 1 \\ \log(1 -f(x)) \;\;\;\;\;\ \text{if} \; Y = 0 \end{cases}\] <p>where \(f\) is the predicted probability and \(x\) is the input data.</p> <p>This is negative BCE! As claimed, it turns out that minimization of binary cross-entropy loss (or maximization of negative BCE) encourages not only the correctness of classification \(f(x)\), but also the truthfulness of the max-prob confidence \(c(x) = \max (f(x), 1 - f(x))\).</p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we have discussed the concept of scoring rules and their importance in evaluating the performance of probabilistic forecasts. We have defined scoring rules and given examples for proper scoring rules. We have shown that log probability and Brier scoring rules are strictly proper. We have also discussed the relationship between scoring rules and predictive certainty. We have shown that the negative of Binary Cross-Entropy (BCE) loss is a proper scoring rule for max-prob confidence. As a result, we have seen that the minimization of BCE loss encourages not only the correctness of classification, but also the truthfulness of the max-prob confidence. Negative Log-Likelihood (NLL), which is the same as Cross Entropy Loss (they have slightly different input expectations in PyTorch, so be careful), is also a proper scoring rule for max-prob confidence. It is a popular metric for evaluating predictive uncertainty as well (<a href="https://arxiv.org/pdf/1612.01474.pdf">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>).</p> <p><strong>References:</strong> Bálint Mucsányi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): “Proper/Strictly Proper Scoring Rule”; in: Trustworthy Machine Learning; URL: https://trustworthyml.io/; DOI: 10.48550/arXiv.2310.08215.</p>]]></content><author><name></name></author><category term="computer-science"/><category term="machine-learning"/><summary type="html"><![CDATA[Table of Contents Introduction Definitions Scoring Rule Proper Scoring Rule Strictly Proper Scoring Rule Scoring Rules Log Probability Brier Score Proofs Theorem (Log Probability Scoring Rule) Theorem (Brier Scoring Rule) Application: Predictive Certainty Max-Prob Confidence BCE Loss Conclusion]]></summary></entry><entry><title type="html">A Short Anthology</title><link href="https://karahans.github.io/blog/20240805" rel="alternate" type="text/html" title="A Short Anthology"/><published>2024-01-15T11:12:00+00:00</published><updated>2024-01-15T11:12:00+00:00</updated><id>https://karahans.github.io/blog/post4</id><content type="html" xml:base="https://karahans.github.io/blog/20240805"><![CDATA[<h3 id="table-of-contents">Table of Contents</h3> <ul> <li><a href="#introduction">Introduction</a></li> <li><a href="#poems">Poems</a> <ul> <li><a href="#clown-in-the-moon">Clown in the Moon</a></li> <li><a href="#do-not-go-gentle-into-that-good-night">Do Not Go Gentle into That Good Night</a></li> <li><a href="#bluebird">Bluebird</a></li> <li><a href="#ne-i̇çindeyim-zamanın">Ne İçindeyim Zamanın</a></li> <li><a href="#ceviz-ağacı">Ceviz Ağacı</a></li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <p>I can’t really say that I’m a huge poetry fan, but I do enjoy reading a good poem every now and then. Recently, I came across a short antology shared by Martin Porter - creator of the <a href="https://tartarus.org/martin/PorterStemmer/index.html">Porter Stemming Algorithm</a>. Inspired by the idea of compiling favorite poems (perhaps fueled by a sense of nostalgia evoked by its extremely simplistic website), I decided to share an anthology of my own. Here are a few of my favourite poems, enjoy!</p> <h2 id="poems">Poems</h2> <h3 id="clown-in-the-moon">Clown in the Moon</h3> <blockquote> <p>My tears are like the quiet drift <br/> Of petals from some magic rose; <br/> And all my grief flows from the rift <br/> Of unremembered skies and snows.</p> <p>I think, that if I touched the earth, <br/> It would crumble; <br/> It is so sad and beautiful, <br/> So tremulously like a dream. <br/>                                                   — <cite>Dylan Thomas</cite></p> </blockquote> <p>I wanted to put two poems from Dylan Thomas in this anthology, <em>Clown in the Moon</em> and <em>Do Not Go Gentle into That Good Night</em>. The latter is quite famous due to its inclusion in the movie Interstellar, from which I also happened to hear it first. <em>Clown in the Moon</em> is a lesser known poem, but it’s one of my favorites. I think it depicts a desperate clown in the moon, looking down at the earth and feeling a sense of sadness and beauty. Given the absurdity of the image (why would a clown be in the moon?), I think it incorporates a little bit of surrealism and romanticism. I must say, I just love how the Earth is portrayed as something so fragile and beautiful with strong wording in play. There is also a great translation of this poem to Turkish, <a href="https://www.siirparki.com/dylan2.html">here</a> by Vehbi Taşar.</p> <h3 id="do-not-go-gentle-into-that-good-night">Do Not Go Gentle into That Good Night</h3> <blockquote> <p>Do not go gentle into that good night, <br/> Old age should burn and rave at close of day;<br/> Rage, rage against the dying of the light.</p> <p>Though wise men at their end know dark is right,<br/> Because their words had forked no lightning they<br/> Do not go gentle into that good night.</p> <p>Good men, the last wave by, crying how bright<br/> Their frail deeds might have danced in a green bay,<br/> Rage, rage against the dying of the light.</p> <p>Wild men who caught and sang the sun in flight,<br/> And learn, too late, they grieved it on its way,<br/> Do not go gentle into that good night.</p> <p>Grave men, near death, who see with blinding sight<br/> Blind eyes could blaze like meteors and be gay,<br/> Rage, rage against the dying of the light.</p> <p>And you, my father, there on the sad height,<br/> Curse, bless, me now with your fierce tears, I pray.<br/> Do not go gentle into that good night.<br/> Rage, rage against the dying of the light.<br/>                                                   — <cite>Dylan Thomas</cite></p> </blockquote> <p>This one really gives me the goosebumps. An uprising against the inevitable could not have been expressed more elegantly and strikingly than this. Not only the content, but the selection of the words and the rhythm of the poem is just perfect. With the emotional intensity at the forefront, this is one of the great examples of romanticism in poetry. Yet, I have lots of question marks in my mind about the imagery and the meaning of the metaphors. What does it mean that “wise men know the dark is right” although “their words had forked no lightning” (maybe their words had no impact on the world)? Maybe it’s about the leaders who failed to make a change in the world, and now they are facing the end of their lives. Final part of the poem is also quite emotional, as the poet is addressing his father and asking him to fight against the dying of the light. The usage of contrast between “curse” and “bless” is quite effective, as the writer desires his father to take some action before departing - curse, bless, just do something. A similar contrast is observed between “light” and “night” throughout the poem.</p> <h3 id="bluebird">Bluebird</h3> <blockquote> <p>there’s a bluebird in my heart that <br/> wants to get out<br/> but I’m too tough for him,<br/> I say, stay in there, I’m not going<br/> to let anybody see<br/> you.<br/> there’s a bluebird in my heart that<br/> wants to get out<br/> but I pour whiskey on him and inhale<br/> cigarette smoke<br/> and the whores and the bartenders<br/> and the grocery clerks<br/> never know that<br/> he’s<br/> in there.</p> <p>there’s a bluebird in my heart that<br/> wants to get out<br/> but I’m too tough for him,<br/> I say,<br/> stay down, do you want to mess<br/> me up?<br/> you want to screw up the<br/> works?<br/> you want to blow my book sales in<br/> Europe?<br/> there’s a bluebird in my heart that<br/> wants to get out<br/> but I’m too clever, I only let him out<br/> at night sometimes<br/> when everybody’s asleep.<br/> I say, I know that you’re there,<br/> so don’t be<br/> sad.<br/> then I put him back,<br/> but he’s singing a little<br/> in there, I haven’t quite let him<br/> die<br/> and we sleep together like<br/> that<br/> with our<br/> secret pact<br/> and it’s nice enough to<br/> make a man<br/> weep, but I don’t<br/> weep, do<br/> you?<br/>                                                   — <cite>Charles Bukowski</cite></p> </blockquote> <p>I know, this one is quite depressive. Notice that the complexity of the words have decreased significantly compared to the previous two poems. Simple words, seemingly random line breaks, and dialogues. This is a poem by Charles Bukowski, a modernist poet, who is known for his raw and unfiltered style. Ever since I heard this poem, the concept of “bluebird” was extremely captivating for me. For most, it is not something easy to achieve. It is not like buying something that you wanted to buy for a while, or developing a simple habit. It is complicated, it is way more than that and it requires sacrifice. It requires a sacrifice for Bukowski, and thinking about all the book sales in Europe, he resigns.</p> <h3 id="ne-i̇çindeyim-zamanın">Ne İçindeyim Zamanın</h3> <blockquote> <p>Ne içindeyim zamanın, <br/> Ne de büsbütün dışında;<br/> Yekpâre, geniş bir ânın<br/> Parçalanmaz akışında.</p> <p>Bir garip rüyâ rengiyle<br/> Uyuşmuş gibi her şekil,<br/> Rüzgârda uçan tüy bile<br/> Benim kadar hafif değil.</p> <p>Başım sükûtu öğüten<br/> Uçsuz, bucaksız değirmen;<br/> İçim muradına ermiş<br/> Abasız, postsuz bir derviş;</p> <p>Kökü bende bir sarmaşık<br/> Olmuş dünya sezmekteyim,<br/> Mavi, masmavi bir ışık<br/> Ortasında yüzmekteyim…<br/>                                                   — <cite>Ahmet Hamdi Tanpınar</cite></p> </blockquote> <p>This is a poem by Ahmet Hamdi Tanpınar, a Turkish writer and poet who lived from 1901 to 1962. I like this poem because of both the individualist and societal aspects it captures. Having witnessed the transformation of Turkey with the establishment of Republic, Tanpınar had focused on the transition process and its effects on the society in his writing as well. In his book, “<em>Saatleri Ayarlama Enstitüsü</em>” (an humble attempt to translate it into English: The Time Regulation Institute), he portrays the Turkish society stuck between East and West, struggling on boths sides to find its own identity. Concept of time is a recurring theme in his works, and this poem is no exception. “Ne içindeyim zamanın, ne de büsbütün dışında” (I am neither inside the time, nor completely outside of it) is a great example of this. “Yekpare geniş bir anın parçalanmaz akışında” (In the unbreakable flow of a single, wide moment) means that the author is not in the past (east) or in the future (west) - because the past and the future is an unbreakable whole. Compared to all the other poems in this anthology, I believe this one is the most abstract and symbolic. From an individualistic perspective, I love how the poet describes himself as a dervish who has reached his goal without any need for a robe or a staff. Third paragpraph as a whole provides a great imagery of the one’s tranquility.</p> <h3 id="ceviz-ağacı">Ceviz Ağacı</h3> <blockquote> <p>Başım köpük köpük bulut, içim dışım deniz,<br/> ben bir ceviz ağacıyım Gülhane Parkı’nda,<br/> budak budak, şerham şerham ihtiyar bir ceviz.<br/> Ne sen bunun farkındasın, ne polis farkında.</p> <p>Ben bir ceviz ağacıyım Gülhane Parkı’nda.<br/> Yapraklarım suda balık gibi kıvıl kıvıl.<br/> Yapraklarım ipek mendil gibi tiril tiril,<br/> koparıver, gözlerinin, gülüm, yaşını sil.<br/> Yapraklarım ellerimdir, tam yüz bin elim var.<br/> Yüz bin elle dokunurum sana, İstanbul’a.<br/> Yapraklarım gözlerimdir, şaşarak bakarım.<br/> Yüz bin gözle seyrederim seni, İstanbul’u.<br/> Yüz bin yürek gibi çarpar, çarpar yapraklarım.<br/> Ben bir ceviz ağacıyım Gülhane Parkı’nda.<br/> Ne sen bunun farkındasın, ne polis farkında.<br/>                                                   — <cite>Nazım Hikmet</cite></p> </blockquote> <p>This one is rather quite famous among Turkish poems. It was written by Nazım Hikmet (1902-1963), who had experienced the same era as Ahmet Hamdi Tanpınar. This one is actually a quite political one as well, although it is not the reason why I included it here. I just love the imagery and the symbolism in this poem. The poet starts with a great line (that I believe to be the most striking over all), “Başım köpük köpük bulut, içim dışım deniz” (humble translation but not really what it is: My head is a foamy cloud, my inside and outside is a sea). This verse gives me a feeling of being dislocated. Both clouds and sea are unstable, going to and fro. Author describes himself as an old walnut tree in the Gülhane Park (quite a lovely place in historical peninsula). He looks at Istanbul, touches and feels it. There are different speculations as to the story of this poem, but apart from it, I think it is a great example of symbolism and imagery.</p>]]></content><author><name></name></author><category term="literature"/><category term="poetry"/><summary type="html"><![CDATA[Table of Contents Introduction Poems Clown in the Moon Do Not Go Gentle into That Good Night Bluebird Ne İçindeyim Zamanın Ceviz Ağacı]]></summary></entry></feed>