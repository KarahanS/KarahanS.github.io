---
layout: page
permalink: /blog/20240324
title: Bayesian Machine Learning in Practice
date: 2024-03-24 11:12:00-0400           # NAME OF THE FILE MUST BE IN THIS FORMAT: date-xxx.md
tags: machine learning
categories: machine learning
nav: true
---

## Bayesian Approach

- What is the difference between *Deterministic ML* and *Bayesian ML*?
    - Deterministic ML first optimizes a single model over the training set, $\theta^*(\mathcal{D})$ where $\mathcal{D}$ is the training data (a collection of $(Y_i, X_i)$). Then for a test sample, it predicts the label as follows:
        
        $P(y | x, \mathcal{D}) = P(y | x, \theta^*(\mathcal{D}))$ 
        
    - We use only this single model to produce the output for input of interest. From the Bayesian perspective, this is equivalent to having a Dirac posterior.
    - As epistemic uncertainty
    arises from the existence of multiple plausible models, but we only consider a single one in deterministic ML, we cannot represent epistemic uncertainty using deterministic ML.
    
    - Posterior distribution (Bayesian ML):
    
    ![Different set of parameters maximizes the probability, meaning that there are more than one optimum models for the task - at least based on the given training set (epistemic uncertainty).](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/a0de3592-e099-44e7-b36a-801885967d42/Untitled.png)
    
    Different set of parameters maximizes the probability, meaning that there are more than one optimum models for the task - at least based on the given training set (epistemic uncertainty).
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/7d402bef-1b4f-4222-829b-f5043546509b/Untitled.png)
    

Bayesian ML finds a distribution of models: $p(\theta | \mathcal{D})$

Then the prediction for a specific test sample is made through **Bayesian Model Averaging** (BMA)/marginalization: 

$P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} =  \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)$

The idea can be simplified as follows: Let‚Äôs say there are $3$ different models (weights) with $0.4, 0.4$, and $0.2$ probabilities. Models output predicted probability as $0.3, 0.25, 0.4$ respectively. Then the resulting probability becomes: $0.4 \times 0.3 + 0.4 \times 0.25 + 0.2 \times 0.4$ = $0.3$.


---

Recipe:

1. Find a clever way to calculate (approximate) posterior distribution $p(\theta | \mathcal{D})$. There are lots of ways listed below.
2. Using the posterior, get the prediction using **Bayesian Model Averaging.** Basically, we are calculating the expected value of the prediction based on the distribution $\theta \sim p(\theta | \mathcal{D})$. Here we can use **Monte Carlo Simulation** to calculate the expected value easily.
    
    $P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)$ 


---

<aside>


Derivation of BMA (Bayes‚Äô Theorem with the observations from DAG :                                                                                                  Observe that $p(a) = \int p(a,b) db$                                                 $P(y | x, \mathcal{D}) = \int P(y, \theta | x,  \mathcal{D}) d\theta = \int P(y | \theta , x,  \mathcal{D}) P(\theta | x, \mathcal{D}) d\theta  = \int P(y | \theta , x) P(\theta | \mathcal{D}) d\theta = \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)$

![Note that $w = \theta$ here, the application I used for drawing didn‚Äôt let me use $\theta$. Test label $y$  is independent of the training data given the parameters $w$. Also, $w$ is independent of the test input $x$. But be careful! It becomes dependent on it when the $y$ is given. You can think of it as two independent dice throws - where $y$ represents the event = sum of the throws is equal to 4.](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/1d7c3da8-8e4e-4f55-9925-12ba59571b26/Untitled.png)

Note that $w = \theta$ here, the application I used for drawing didn‚Äôt let me use $\theta$. Test label $y$  is independent of the training data given the parameters $w$. Also, $w$ is independent of the test input $x$. But be careful! It becomes dependent on it when the $y$ is given. You can think of it as two independent dice throws - where $y$ represents the event = sum of the throws is equal to 4.

</aside>

## Bayesian ML Approach #1: Ensemble

So, we can utilize Bayesian ML to formulate epistemic uncertainty. Great, but how can we produce the probability distribution $P(\theta|D)$? One way of doing it is using ensemble approach - where we train a model $M$  times with different random seeds. We can directly use the original training set or sample with replacement from it for each training. Here, we train several deterministic models. Therefore at the end, our distribution $P(\theta|D)$  simply becomes the sum of Dirac measures*.

<aside>
üìñ Dirac measure, given a measurable space $(\Omega, \mathcal{B})$ and $w \in \Omega$, is a function $\delta_w: \mathcal{B} \rightarrow \Omega$, defined as follows:     $\delta_w(A) =  \begin{cases}
                        1, \; \text{if $\omega \in A$} \\
                        0, \; \text{if $\omega \notin A$}
                    \end{cases}$

There is also the Dirac delta function (not formally a function) 

$\delta(x)= \begin{cases}
\infty, \; \text{if $x = 0$} \\
0, \; \text{if $x \neq 0$}
\end{cases}$

The function¬†$Œ¥(x)$  has the value zero everywhere except at¬†$*x¬†= 0*$, where its value is infinitely large and is such that its total integral is $1$.

[Dirac function is commonly used to represent discrete probability distributions in a continuous way.](https://en.wikipedia.org/w/index.php?title=Probability_distribution#Dirac_delta_representation)

As a probability distribution, integral of the Dirac function over the whole domain gets equal to $1$.

$\int_{-\infty}^\infty \delta(x) dx = 1$

One final property that is extremely important is the following:

$\int_{-\infty}^{\infty} f(x) \delta(x - c) \,dx = f(c)$

There is a proof for this property [here](https://math.stackexchange.com/questions/73010/proof-of-dirac-deltas-sifting-property). But  I must say, this is one of the key characteristics of Dirac function and there is not a very convincing proof (think of it as an axiom rather than a conclusion).

</aside>

![When we train ensemble of $M$  models, we basically get the sum of Dirac measures as our probability distribution of the random variable $\theta$. Each Dirac measure is given as $\delta(\theta - \theta_m)$.](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/5894a6e8-1729-4561-8ef7-ab1cf9d7a028/Untitled.png)

When we train ensemble of $M$  models, we basically get the sum of Dirac measures as our probability distribution of the random variable $\theta$. Each Dirac measure is given as $\delta(\theta - \theta_m)$.

$P(\theta|D) = \frac{1}{M} \sum_m \delta(\theta - \theta_m)$

- ‚ÄúSum of diracs‚Äù should be a probability distribution, therefore its integral over real numbers must be equal to 1.
    
     $\int_{-\infty}^\infty \frac{1}{M} \sum_m \delta(\theta - \theta_m)d\theta \\ = \frac{1}{M} \sum_m \int_{-\infty}^\infty \delta(\theta - \theta_m)d\theta \\= \frac{1}{M} \sum_m \int_{-\infty}^\infty \delta(y)dy \\=   \frac{1}{M} \sum_m  1 \\= \frac{1}{M} M \\= 1$ 
    
    Now, we can insert our posterior to the Bayesian Model Averaging:
    
    $P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} \\  \;\;\; \;\;\; \;\;\; \;\;\; \;\;\;\; = \int{P(y | x, \theta) \; \frac{1}{M} \sum_m \delta(\theta - \theta_m)   \; d\theta}   \\  \;\;\; \;\;\; \;\;\; \;\;\; \;\;\;\; = \frac{1}{M} \sum_m \int{P(y | x, \theta) \delta(\theta - \theta_m)   \; d\theta} \\  \;\;\; \;\;\; \;\;\; \;\;\; \;\;\;\; = \frac{1}{M} \sum_m P(y | x, \theta_m)$
    
    So, as a result, we can get the prediction by taking the average of the resulting predictions from different models.
    
    - Pros
    ‚óè Conceptually simple - run training algorithm $M$ times.
    ‚óè Applicable to a wide range of models - from Linear Regression to LLMs.
    ‚óè Embarrassingly parallel - easy to parallelize.
    - Cons
    ‚óè Perhaps not realizing the full potential of Bayesian ML (only finite #models).
    ‚óè Space & time complexities scale linearly with $M$ (#models).
    

## Bayesian ML Approach #2: Training Infinite Number of Models (Bayes by Backprop)

Of course, it is not practically possible to train ‚Äúinfinite‚Äù number of models. But think about this: you can represent a probability distribution of infinite number of random variables with finite number of parameters (mean and variance). This is what we‚Äôll try to do here.

So, we can approximate $P(\theta|\mathcal{D})$ simply by a Gaussian distribution: 

 $P(\theta|\mathcal{D}) \sim \mathcal{N}(\theta | \mu^*(\mathcal{D}), \Sigma^*(\mathcal{D}))$ (Here since $D$ consists of all inputs, it is multivariate Gaussian)

However, we don‚Äôt really know if the posterior follows a Gaussian distribution. We can try to find the closest Gaussian distribution to the actual distribution of the posterior. We would like to minimize the distance between the two distributions. Which metric quantifies how much one probability distribution differs from another probability distribution? KL-Divergence!

$\text{min}_{\mu, \Sigma} d(\mathcal{N}(\theta | \mu(\mathcal{D}), \Sigma(\mathcal{D})), p(\theta | \mathcal{D}))$

Now, let‚Äôs derive the training objective. 

$\text{KL}(\mathcal{N}(\theta | \mu, \Sigma) \; || \; p(\theta | \mathcal{D})) = \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \Sigma) \log(\frac{\mathcal{N}(\theta | \mu, \Sigma)}{p(\theta | \mathcal{D}))}) d\theta \\$ Now, let‚Äôs first deal with the denominator of natural logarithm.

$p(\theta | \mathcal{D}) = \frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})}$  We can consider $\ln(\frac{1}{p(\mathcal{D})}) = - \ln(p(\mathcal{D}))$ as a constant.

$= \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \Sigma) \log(\frac{\mathcal{N}(\theta | \mu, \Sigma)}{p(\mathcal{D} | \theta) p(\theta))}) d\theta + C_1$

Let‚Äôs separate the logarithm:

$= \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \Sigma) \log(\frac{\mathcal{N}(\theta | \mu, \Sigma)}{p(\theta)}) d\theta - \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \Sigma) \log(  p(\mathcal{D} | \theta)) d\theta + C_1\\ = \text{KL}(\mathcal{N}(\theta | \mu, \Sigma) \; || \; p(\theta)) -  \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \Sigma) \log(  p(\mathcal{D} | \theta)) d\theta + C_1\\ = \text{KL}(\mathcal{N}(\theta | \mu, \Sigma) \; || \; p(\theta)) -  \mathbb{E}_{\theta \sim \mathcal{N}(\theta | \mu, \Sigma)} \log(p(\mathcal{D}|\theta)) d\theta + C_1$

Ok so far so good, now assume that the input data is *identically independently distributed* (each $(Y_i, X_i)$ are independent from the other). We can represent $p(\mathcal{D}|\theta)$ as follows: $p(\mathcal{D}|\theta) = \prod_{i} p(Y_i, X_i|\theta)$ When you take the logarithm: $\ln(p(\mathcal{D}|\theta)) = \sum_{i} \ln( p(Y_i, X_i|\theta))$

Therefore the expectation reduces to the following:

$\mathbb{E}_{\theta \sim \mathcal{N}(\theta | \mu, \Sigma)} \log(p(\mathcal{D}|\theta)) d\theta = \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i, X_i|\theta)) d\theta$

Let‚Äôs play with the probabilities here as well:

$p(Y_i, X_i|\theta) = \frac{p(Y_i, X_i, \theta)}{p(\theta)} =\frac{p(Y_i | X_i, \theta) p(X_i, \theta)}{p(\theta)} =  \frac{p(Y_i | X_i, \theta) p(X_i | \theta) p(\theta)}{p(\theta)} =  p(Y_i | X_i, \theta) p(X_i | \theta) = p(Y_i | X_i, \theta) p(X_i) = p(Y_i | X_i, \theta) \; C'$  

$\theta$ and $X_i$ are independent as long as $Y_i$ is not provided. Now, insert this result to the previous expectation:

$\sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i | X_i, \theta) C') d\theta = \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i | X_i, \theta)) d\theta +   \log(C')  \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} d\theta = \sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i | X_i, \theta)) d\theta + C_2$

Expected value of a constant is equal to that constant.

Ok now, there is a method called **Monte Carlo Simulation.** Unnecessarily fancy name, but basically it says the following: To estimate the expected value of a function $f(\theta)$ under the distribution $\theta \sim \mathcal{N}$, simply just run the experiment $K$ times and take the average value of $f(\theta)$ over runs. We can approximate the last equation as follows with MC simulation:

$\sum_i \mathbb{E}_{\theta \sim \mathcal{N}} \log( p(Y_i | X_i, \theta)) d\theta =   \sum_i \frac{1}{K} \sum_k  \log(p(Y_i | X_i, \theta_k))$  where $\theta_k \sim \mathcal{N}(\mu, \Sigma)$ 

So, the final objective function becomes the following:

$min_{\mu, \Sigma} \; \text{KL}(\mathcal{N}(\theta | \mu, \Sigma) \; || \; p(\theta)) - \frac{1}{K}  \sum_i \sum_k \log(p(Y_i | X_i, \theta_k)) + C$ 

Right part can be calculated by simply running some experiments. For the left part, what is prior $p(\theta)$? This represents our initial idea as to the probability distribution for model parameters. Popular choice for the posterior is just a standard normal distribution: $p(\theta) = \mathcal{N}(\vec{0}^{\,}, {I})$ (multivariate)

Another observation here is that allowing a full covariance matrix $\Sigma$ can cause the computation to take extremely long (there are lots of possibilities). One trick is restricting the covariance matrix to be diagonal (independent features) $\rightarrow \Sigma = \text{diag}(\sigma^2)$ where $\sigma^2$ is the variance vector for each feature $(\text{Var}[\theta^i])$. ($\theta_i$ denotes the  $i^{\text{th}}$ set of parameters sampled from the Gaussian distribution. $\theta^i$ denotes the $i^{\text{th}}$ dimension of the vector of parameters)

$\text{KL}(\mathcal{N}(\theta | \mu, \text{diag}(\sigma^2)) \; || \; \mathcal{N}(\vec{0}^{\,}, {I})) = \int_{-\infty}^{\infty} \mathcal{N}(\theta | \mu, \text{diag}(\sigma^2))\log(\frac{\mathcal{N}(\theta | \mu, \text{diag}(\sigma^2))}{\mathcal{N}(\vec{0}^{\,}, {I})}) d\theta \\$ So, this equation boils down to the following:

$= \sum_i [\frac{\mu_i^2}{\sigma_i^2} + \log\sigma_i - \frac{1}{2}]$  ‚áí this behaves like a regularizer for $\mu$ and $\sigma$

Multivariate Gaussian distribution for $k$ features ($k$ dimensional $\theta$ vector): 

$\mathcal{N}(\mu, \Sigma) = (2\pi)^{-k/2} \text{det}(\Sigma)^{-1/2} \text{exp}(-\frac{1}{2} (x-\mu)^T \Sigma^{-1}(x - \mu))$

for $\mathcal{N}(\vec{0}^{\,}, {I}) = (2\pi)^{-k/2} \text{exp}(-\frac{1}{2}x^Tx)$

$min_{\mu, \Sigma} \;  \sum_i [\frac{\mu_i^2}{\sigma_i^2} + \log\sigma_i - \frac{1}{2}] - \frac{1}{K} \sum_i \sum_k \log(p(Y_i | X_i, \theta_k)) + C$ where $\theta_k \sim \mathcal{N}(\mu, \text{diag}(\sigma^2))$ 

Now, our equations are not still not Neural Network friendly - because we have to try to minimize a function that is not directly dependent on $\mu$ and $\Sigma$. We have to represent $\theta$  to include both $\mu$ and $\Sigma$. We can use the reparameterization here.

<aside>

If $X \sim \mathcal{N}(\mu, \sigma^2)$, then $Z = \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)$
Since we set $\Sigma$ to be $\text{diag}(\sigma^2)$, all dimensions of the parameter vector $\theta$ are independent of each other. Individually $\theta^i \sim \mathcal{N}(\mu_i, \sigma_i^2)$ - 1D Gaussian random variable. Therefore we can perform the given parameterization trick.

</aside>

$\theta = \mu + \sigma \odot \epsilon$ where $\odot$ is the pointwise multiplication and $\epsilon \sim \mathcal{N}(\vec{0}^{\,}, {I})$

This reparameterization makes the backpropagation extremely simple:

$\frac{\partial \mathcal{L}}{\partial \mu_i} = \sum_j \frac{\partial \mathcal{L}}{\partial \theta_j} \frac{\partial \theta_j}{\partial \mu_i}  = \frac{\partial \mathcal{L}}{\partial \theta_i}$ 

$\frac{\partial \mathcal{L}}{\partial \sigma_i} = \sum_j \frac{\partial \mathcal{L}}{\partial \theta_j} \frac{\partial \theta_j}{\partial \sigma_i} = \epsilon_i \frac{\partial \mathcal{L}}{\partial \theta_i}$

Usually we set $K$ to $1$, that is, we only sample one $\theta$ from the distribution **in each iteration**. Then the second term simply becomes the cross-entropy loss:

$- \sum_i  \log(p(Y_i | X_i, \theta))$ = observe that this is very similar to the cross-entropy loss (for a group of data points - not a single one, don‚Äôt get confused). There is one single addition we have to do: averaging

$\frac{1}{N} \sum_i \log(p(Y_i | X_i, \theta))$ calculates the cross entropy loss ($P(Y_i) = 1$) for a batch of $(Y_i, X_i)$  pairs.

One final note: Standard deviation $\sigma$ is the square root of variance $\sigma^2$ and therefore it must be non-negative. However, PyTorch doesn‚Äôt know this and it might produce negative $\sigma$ as a result of the backpropagation. We can define another function for $\sigma$ with an input allowed to be negative:

$\sigma = \ln(1+e^\rho)$ (softplus - $\sigma$ is always non-negative, let the model update the value of $\rho$)

PyTorch code for ‚ÄúTraining Infinite Number of Models‚Äù ($K$ is set to $1$, therefore only one $\epsilon$ is created):

![In the forward function, we multiply $\theta$ with $x$. Here the idea is to mimic a shallow NN consisting of input and output layers. Result of the multiplication gives us the logits (activation must be applied to get the predicted probabilities) The main idea in this code is this: Rather than training one parameter set $\theta$ over each iteration, we train parameters \mu and \sigma that will generate infinitely many $\theta$ ‚Üí Bayes ML](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/369d8928-12a9-4bf9-8897-9179d2c48ef7/Untitled.png)

In the forward function, we multiply $\theta$ with $x$. Here the idea is to mimic a shallow NN consisting of input and output layers. Result of the multiplication gives us the logits (activation must be applied to get the predicted probabilities) The main idea in this code is this: Rather than training one parameter set $\theta$ over each iteration, we train parameters \mu and \sigma that will generate infinitely many $\theta$ ‚Üí Bayes ML

As a result of this model, we will get $\mu^*$ and  $\Sigma^*$ that minimizes the $\text{KL}(\mathcal{N}(\theta | \mu(\mathcal{D}), \Sigma(\mathcal{D})), p(\theta | \mathcal{D}))$ where $P(\theta|\mathcal{D}) \sim \mathcal{N}(\theta | \mu^*(\mathcal{D}), \Sigma^*(\mathcal{D}))$

What remains is the application of BMA. Calculate the expected value of $P(y |x, \theta)$ using the $\theta$ sampled from $\mathcal{N}(\theta | \mu^*(\mathcal{D}), \Sigma^*(\mathcal{D}))$ using the learned mean and variance.

$P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{P(\theta|D)} P(y |x, \theta)$

## Bayesian ML Approach #3: Dropout

On the spectrum of Bayesian methods, dropout is between the sum of Diracs (training a few deterministic ML models) and the variational approach (that trains an infinite number of models). 

Dropout is randomly applied to the model **for each iteration.** Considering the Cross Entropy Loss, objective function of the model can be formulated as follows:

$-\frac{1}{N}\sum_{n=1}^{N}\log P(y_n | x_n, s \odot \theta)$ where we turn on/off each weight dimension randomly in each iteration: $s_i \sim \text{Bernouilli}( p)$ where $p$ is the dropout rate. Dropout is something that people used to use a lot ‚Üí Bayesian NN for free.

Now, here is the idea: Normally, we train models with dropouts and then close the dropout at inference time (test time). Here, we can still apply dropout to have different models make predictions during test time. Again, we apply Bayesian Model Averaging with Monte Carlo estimation for the integral. 

$P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} \\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \mathbb{E}_{\theta \sim P(\theta|D)} P(y |x, \theta) \\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \approx \frac{1}{K} \sum_{k=1}^K P(y | x, \theta_{k})  \;\; \text{where} \;\; \theta_{k} = s_k \odot \theta$

There are $K$ models to compare with each other. We expect to have disagreements between the models for OOD samples, and therefore measure the epistemic uncertainty. 

Problem: Unfortunately, this method doesn‚Äôt work that well in practice:

![R: Random signed vector added - AT: Adversarial training added ](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/5f65092c-35dc-409a-b6d0-ae2865944072/Untitled.png)

R: Random signed vector added - AT: Adversarial training added 

## Bayesian ML Approach #4: Training a Curve of an Infinite Number of Models

This idea makes the following assumption: If you train two models independently, you get two different $\theta_i$ weights that are far away from each other in the space. But from one to another, there is a path where the loss value is always low. So we can find the weights in between to have infinite number of models. 

This is also Bayesian, as we are training an infinite number of models according to a learned parametric approximate posterior.

Here is the algorithm to train a curve of infinite number of models:

1. Train two independent models: $\theta_1$ and $\theta_2$
2. Parameterize the curve via a third model $\phi$. There are two line segments between the models.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/e8e234b0-131f-4e4d-83ad-b8cb9174d429/Untitled.png)

$\theta_{\phi}(t) = \begin{cases}  2(t\phi + (0.5-t)\theta_1)\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if}\; t \in [0, 0.5) \\ 2((t - 0.5)\theta_2 + (1-t)\phi) \;\;\;\; \text{if} \; t \in [0.5, 1] \end{cases}$

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/8dee073f-f9e2-4cf6-add9-61cbf6992a8a/Untitled.png)

$\theta_1$ and $\theta_2$ are already known, we just have to find $\phi$ so that the any model on the curve has low training loss. So, we actually have to minimize the expected value of the cross entropy loss:

$\text{min}_\phi \;\mathbb{E}_{\theta \sim q_{\phi}(\theta)} \left[ -\frac{1}{N} \sum_n \log P(y_n | x_n, \theta) \right]$  ‚Üí find a suitable \phi under the expectation of both $\theta$ and $X, Y$

Using the reparameterization trick:

$\text{min}_\phi \;\mathbb{E}_{t \sim \text{Unif}[0, 1]}\left[ -\frac{1}{N} \sum_n \log P(y_n | x_n, \theta_{\phi}(t)) \right]$

This minimization aims to minimize the cross entropy loss by finding a suitable $\phi$.

> A general observation is that almost all pairs of independently trained models ($\theta_1, \theta_2$) for DNNs are connected through a third point $\phi$ in a low-loss ‚Äúhighway‚Äù that we can easily find. This gives an interesting intuition for the loss landscape: Most solutions in the DL landscape are connected by some piecewise linear curve. This is not so surprising: We have millions/billions of dimensions to choose from. We can likely find a 2D cut of the loss in which there exists a parametric curve parameterized by $\phi$  that connects the two endpoints with a low training loss.
> 

After training the model and finding a suitable $\phi$, the rest is using the **Bayesian Model Averaging** by sampling models (weights) from the curve between $\theta_1$ and $\theta_2$:

$P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} \\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \mathbb{E}_{\theta \sim P(\theta|D)} P(y |x, \theta) \\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \approx \frac{1}{K} \sum_{k=1}^K P(y | x, \theta_\phi{(t^{(k)})}) \;\; \text{where} \;\; t^{(k)} \sim   \text{Unif}[0, 1]$

## Bayesian ML Approach #5: Stochastic Weight Averaging (SWAG)

Lastly, we can exploit the randomness in SGD as a cheap source for Bayesian ML. Because of small batches (usually batch size = 1), SGD trajectory is very noisy. The training‚Äôs final few iterations (epochs) can be treated as samples from the approximate posterior distribution $p(\theta | \mathcal{D})$. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/2364d26c-298c-444c-8fc2-37fa79ae659e/7211ef32-936c-42f1-b37a-59351edcf166/Untitled.png)

We would like to approximate the last iterations with Gaussian:

$p(\theta | \mathcal{D}) \sim \mathcal{N}(\theta | \mu(\mathcal{D}), \Sigma(\mathcal{D}))$

We assume that last $\theta_i$ values are coming from a Gaussian distribution. We want to find the mean and the covariance matrix for it. We can use the sample estimates basically:

$\mu = \frac{1}{L} \sum_l \theta_l$

$\Sigma = \text{diag} \left(  \frac{1}{L} \sum_l \theta_l^2 - (\frac{1}{L} \sum_l \theta_l)^2 \right)$ assuming that the different features $\theta_i$  and $\theta_j (i \neq j)$ are not correlated with each other. 

SWAG is not so scalable because it requires the full empirical covariance matrix. On the other hand SWAG-Diag can be used to reduce the time complexity. However, according to the benchmark results, it seems that SWAG-Diag only scales better computationally, but the results do not follow.

After getting the $p(\theta | \mathcal{D})$, rest is similar to what we have done so far:

1. Using the posterior, get the prediction using **Bayesian Model Averaging.** Basically, we are calculating the expected value of the prediction based on the distribution $\theta \sim p(\theta | \mathcal{D})$.
    
    $P(y | x, \mathcal{D}) = \int{P(y | x, \theta) \; P(\theta | \mathcal{D}) \; d\theta} = \mathbb{E}_{\theta \sim P(\theta|\mathcal{D})} P(y |x, \theta)$ 
    
2. To approximate the epistemic uncertainty, either calculate the entropy $\mathbb{H}(P(y | x, \mathcal{D}))$ or the max-prob for classification $\text{max}_k P(Y = k | x, \mathcal{D})$.

---

**Conclusion**: There are strong assumptions for these Bayesian approaches to work:

- We should have a reasonable prior for the parameters $\theta$.
- The posterior follows the assumed distribution (e.g., a Gaussian). Of course, the posterior will seldom be truly Gaussian. This is a huge assumption.

## Epistemic Uncertainty: Non-Bayesian Approach

There is a clever alternative to infinite-number-of-models training: We can measure the *distance* between test sample $X$ and training samples. There are lots of different ideas what this *distance* should be. One way is assuming that the ID data comes from the Gaussian distribution. We expect an OOD data to have a low probability based on the Gaussian distribution we approximate using the data in hand.

<aside>
A **Gaussian mixture model (GMM)** is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.

</aside>

**Represent each input data $x_i$ in the feature space learned by the DNN** (in general, you have to get the output of the second last layer - basically you have to take the output of the last convolutional layer). 

Gaussian model for the class $k$ $\sim \mathcal{N}(\mu_k, \Sigma_k)$

$\mu_k = \frac{1}{N_k} \sum_{i:y_i = k} f(x_i)$  ($x_i$ represents the input data, $f(x_i)$ denotes the feature space representing $x_i$) 

$\Sigma_k = \frac{1}{N_k} \Sigma_{i:y_i = k} (f(x_i) - \mu_k) (f(x_i) - \mu_k)^T$

OODness score = likelihood (GMM density) at $f(x)$

$p(f(x_i)) = \sum_c p(y_i = c) p(f(x_i) | y_i = c)$

$p(y_i = c) = \frac{\text{number of training samples from class} \;c}{\text{total number of training samples}}$

$p(f(x_i) | y_i = c) = \frac{1}{\sqrt{2 \pi \text{det} \Sigma_c}} \text{exp} \left [ - \frac{1}{2} (f(x_i) - \mu_c)^T \Sigma_c^{-1} (f(x_i) - \mu_c) \right ]$

We can put a threshold on the final probability to classify a sample as OOD or not.